<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>LLaVA: Model Zoo</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">LLaVA<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('md_docs_2_m_o_d_e_l___z_o_o.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Model Zoo</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md45"></a></p>
<p><b>To Use LLaVA-1.6 checkpoints, your llava package version must be newer than 1.2.0. <a href="https://github.com/haotian-liu/LLaVA#upgrade-to-latest-code-base">Instructions</a> on how to upgrade.</b></p>
<p>If you are interested in including any other details in Model Zoo, please open an issue :)</p>
<p>The model weights below are <em>merged</em> weights. You do not need to apply delta. The usage of LLaVA checkpoints should comply with the base LLM's model license.</p>
<h1><a class="anchor" id="autotoc_md46"></a>
LLaVA-v1.6</h1>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Version   </th><th class="markdownTableHeadNone">LLM   </th><th class="markdownTableHeadNone">Schedule   </th><th class="markdownTableHeadNone">Checkpoint   </th><th class="markdownTableHeadNone">MMMU   </th><th class="markdownTableHeadNone">MathVista   </th><th class="markdownTableHeadNone">VQAv2   </th><th class="markdownTableHeadNone">GQA   </th><th class="markdownTableHeadNone">VizWiz   </th><th class="markdownTableHeadNone">SQA   </th><th class="markdownTableHeadNone">TextVQA   </th><th class="markdownTableHeadNone">POPE   </th><th class="markdownTableHeadNone">MME   </th><th class="markdownTableHeadNone">MM-Bench   </th><th class="markdownTableHeadNone">MM-Bench-CN   </th><th class="markdownTableHeadNone">SEED-IMG   </th><th class="markdownTableHeadNone">LLaVA-Bench-Wild   </th><th class="markdownTableHeadNone">MM-Vet    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaVA-1.6   </td><td class="markdownTableBodyNone">Vicuna-7B   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b">liuhaotian/llava-v1.6-vicuna-7b</a>   </td><td class="markdownTableBodyNone">35.8   </td><td class="markdownTableBodyNone">34.6   </td><td class="markdownTableBodyNone">81.8   </td><td class="markdownTableBodyNone">64.2   </td><td class="markdownTableBodyNone">57.6   </td><td class="markdownTableBodyNone">70.1   </td><td class="markdownTableBodyNone">64.9   </td><td class="markdownTableBodyNone">86.5   </td><td class="markdownTableBodyNone">1519/332   </td><td class="markdownTableBodyNone">67.4   </td><td class="markdownTableBodyNone">60.6   </td><td class="markdownTableBodyNone">70.2   </td><td class="markdownTableBodyNone">81.6   </td><td class="markdownTableBodyNone">43.9    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">LLaVA-1.6   </td><td class="markdownTableBodyNone">Vicuna-13B   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.6-vicuna-13b">liuhaotian/llava-v1.6-vicuna-13b</a>   </td><td class="markdownTableBodyNone">36.2   </td><td class="markdownTableBodyNone">35.3   </td><td class="markdownTableBodyNone">82.8   </td><td class="markdownTableBodyNone">65.4   </td><td class="markdownTableBodyNone">60.5   </td><td class="markdownTableBodyNone">73.6   </td><td class="markdownTableBodyNone">67.1   </td><td class="markdownTableBodyNone">86.2   </td><td class="markdownTableBodyNone">1575/326   </td><td class="markdownTableBodyNone">70   </td><td class="markdownTableBodyNone">64.4   </td><td class="markdownTableBodyNone">71.9   </td><td class="markdownTableBodyNone">87.3   </td><td class="markdownTableBodyNone">48.4    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaVA-1.6   </td><td class="markdownTableBodyNone">Mistral-7B   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b">liuhaotian/llava-v1.6-mistral-7b</a>   </td><td class="markdownTableBodyNone">35.3   </td><td class="markdownTableBodyNone">37.7   </td><td class="markdownTableBodyNone">82.2   </td><td class="markdownTableBodyNone">64.8   </td><td class="markdownTableBodyNone">60.0   </td><td class="markdownTableBodyNone">72.8   </td><td class="markdownTableBodyNone">65.7   </td><td class="markdownTableBodyNone">86.7   </td><td class="markdownTableBodyNone">1498/321   </td><td class="markdownTableBodyNone">68.7   </td><td class="markdownTableBodyNone">61.2   </td><td class="markdownTableBodyNone">72.2   </td><td class="markdownTableBodyNone">83.2   </td><td class="markdownTableBodyNone">47.3    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">LLaVA-1.6   </td><td class="markdownTableBodyNone">Hermes-Yi-34B   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.6-34b">liuhaotian/llava-v1.6-34b</a>   </td><td class="markdownTableBodyNone">51.1   </td><td class="markdownTableBodyNone">46.5   </td><td class="markdownTableBodyNone">83.7   </td><td class="markdownTableBodyNone">67.1   </td><td class="markdownTableBodyNone">63.8   </td><td class="markdownTableBodyNone">81.8   </td><td class="markdownTableBodyNone">69.5   </td><td class="markdownTableBodyNone">87.7   </td><td class="markdownTableBodyNone">1631/397   </td><td class="markdownTableBodyNone">79.3   </td><td class="markdownTableBodyNone">79   </td><td class="markdownTableBodyNone">75.9   </td><td class="markdownTableBodyNone">89.6   </td><td class="markdownTableBodyNone">57.4   </td></tr>
</table>
<p><em>LLaVA-1.6-34B outperforms Gemini Pro on benchmarks like MMMU and MathVista.</em></p>
<h1><a class="anchor" id="autotoc_md47"></a>
LLaVA-v1.5</h1>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Version   </th><th class="markdownTableHeadNone">Size   </th><th class="markdownTableHeadNone">Schedule   </th><th class="markdownTableHeadNone">Checkpoint   </th><th class="markdownTableHeadNone">VQAv2   </th><th class="markdownTableHeadNone">GQA   </th><th class="markdownTableHeadNone">VizWiz   </th><th class="markdownTableHeadNone">SQA   </th><th class="markdownTableHeadNone">TextVQA   </th><th class="markdownTableHeadNone">POPE   </th><th class="markdownTableHeadNone">MME   </th><th class="markdownTableHeadNone">MM-Bench   </th><th class="markdownTableHeadNone">MM-Bench-CN   </th><th class="markdownTableHeadNone">SEED   </th><th class="markdownTableHeadNone">LLaVA-Bench-Wild   </th><th class="markdownTableHeadNone">MM-Vet    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaVA-1.5   </td><td class="markdownTableBodyNone">7B   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.5-7b">liuhaotian/llava-v1.5-7b</a>   </td><td class="markdownTableBodyNone">78.5   </td><td class="markdownTableBodyNone">62.0   </td><td class="markdownTableBodyNone">50.0   </td><td class="markdownTableBodyNone">66.8   </td><td class="markdownTableBodyNone">58.2   </td><td class="markdownTableBodyNone">85.9   </td><td class="markdownTableBodyNone">1510.7   </td><td class="markdownTableBodyNone">64.3   </td><td class="markdownTableBodyNone">58.3   </td><td class="markdownTableBodyNone">58.6   </td><td class="markdownTableBodyNone">65.4   </td><td class="markdownTableBodyNone">31.1    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">LLaVA-1.5   </td><td class="markdownTableBodyNone">13B   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.5-13b">liuhaotian/llava-v1.5-13b</a>   </td><td class="markdownTableBodyNone">80.0   </td><td class="markdownTableBodyNone">63.3   </td><td class="markdownTableBodyNone">53.6   </td><td class="markdownTableBodyNone">71.6   </td><td class="markdownTableBodyNone">61.3   </td><td class="markdownTableBodyNone">85.9   </td><td class="markdownTableBodyNone">1531.3   </td><td class="markdownTableBodyNone">67.7   </td><td class="markdownTableBodyNone">63.6   </td><td class="markdownTableBodyNone">61.6   </td><td class="markdownTableBodyNone">72.5   </td><td class="markdownTableBodyNone">36.1    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaVA-1.5   </td><td class="markdownTableBodyNone">7B   </td><td class="markdownTableBodyNone">lora-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.5-7b-lora">liuhaotian/llava-v1.5-7b-lora</a>   </td><td class="markdownTableBodyNone">79.1   </td><td class="markdownTableBodyNone">63.0   </td><td class="markdownTableBodyNone">47.8   </td><td class="markdownTableBodyNone">68.4   </td><td class="markdownTableBodyNone">58.2   </td><td class="markdownTableBodyNone">86.4   </td><td class="markdownTableBodyNone">1476.9   </td><td class="markdownTableBodyNone">66.1   </td><td class="markdownTableBodyNone">58.9   </td><td class="markdownTableBodyNone">60.1   </td><td class="markdownTableBodyNone">67.9   </td><td class="markdownTableBodyNone">30.2    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">LLaVA-1.5   </td><td class="markdownTableBodyNone">13B   </td><td class="markdownTableBodyNone">lora-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.5-13b-lora">liuhaotian/llava-v1.5-13b-lora</a>   </td><td class="markdownTableBodyNone">80.0   </td><td class="markdownTableBodyNone">63.3   </td><td class="markdownTableBodyNone">58.9   </td><td class="markdownTableBodyNone">71.2   </td><td class="markdownTableBodyNone">60.2   </td><td class="markdownTableBodyNone">86.7   </td><td class="markdownTableBodyNone">1541.7   </td><td class="markdownTableBodyNone">68.5   </td><td class="markdownTableBodyNone">61.5   </td><td class="markdownTableBodyNone">61.3   </td><td class="markdownTableBodyNone">69.5   </td><td class="markdownTableBodyNone">38.3   </td></tr>
</table>
<p>Base model: Vicuna v1.5. Training logs: <a href="https://api.wandb.ai/links/lht/6orh56wc">wandb</a>.</p>
<p><img src="../images/llava_v1_5_radar.jpg" alt="" width="500px" class="inline"/> <br  />
 LLaVA-1.5 achieves SoTA performance across 11 benchmarks. </p>
<h1><a class="anchor" id="autotoc_md48"></a>
LLaVA-v1</h1>
<p><em>Note: We recommend using the most capable LLaVA-v1.6 series above for the best performance.</em></p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Base LLM   </th><th class="markdownTableHeadNone">Vision Encoder   </th><th class="markdownTableHeadNone">Pretrain Data   </th><th class="markdownTableHeadNone">Pretraining schedule   </th><th class="markdownTableHeadNone">Finetuning Data   </th><th class="markdownTableHeadNone">Finetuning schedule   </th><th class="markdownTableHeadNone">LLaVA-Bench-Conv   </th><th class="markdownTableHeadNone">LLaVA-Bench-Detail   </th><th class="markdownTableHeadNone">LLaVA-Bench-Complex   </th><th class="markdownTableHeadNone">LLaVA-Bench-Overall   </th><th class="markdownTableHeadNone">Download    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v1.3   </td><td class="markdownTableBodyNone">CLIP-L-336px   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-80K   </td><td class="markdownTableBodyNone">proj-1e, lora-1e   </td><td class="markdownTableBodyNone">64.3   </td><td class="markdownTableBodyNone">55.9   </td><td class="markdownTableBodyNone">81.7   </td><td class="markdownTableBodyNone">70.1   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3">LoRA</a> <a href="https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3">LoRA-Merged</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">LLaMA-2-13B-Chat   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-80K   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone">56.7   </td><td class="markdownTableBodyNone">58.6   </td><td class="markdownTableBodyNone">80.0   </td><td class="markdownTableBodyNone">67.9   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview">ckpt</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaMA-2-7B-Chat   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-80K   </td><td class="markdownTableBodyNone">lora-1e   </td><td class="markdownTableBodyNone">51.2   </td><td class="markdownTableBodyNone">58.9   </td><td class="markdownTableBodyNone">71.6   </td><td class="markdownTableBodyNone">62.8   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview">LoRA</a>   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md49"></a>
Projector weights</h1>
<p>These are projector weights we have pretrained. You can use these projector weights for visual instruction tuning. They are just pretrained on image-text pairs and are NOT instruction-tuned, which means they do NOT follow instructions as well as our official models and can output repetitive, lengthy, and garbled outputs. If you want to have nice conversations with LLaVA, use the checkpoints above (LLaVA v1.6).</p>
<p>NOTE: These projector weights are only compatible with <code>llava&gt;=1.0.0</code>. Please check out the latest codebase if your local code version is below v1.0.0.</p>
<p>NOTE: When you use our pretrained projector for visual instruction tuning, it is very important to use the same base LLM and vision encoder as the one we used for pretraining the projector. Otherwise, the performance will be very poor.</p>
<p>When using these projector weights to instruction-tune your LMM, please make sure that these options are correctly set as follows,</p>
<div class="fragment"><div class="line">--mm_use_im_start_end False</div>
<div class="line">--mm_use_im_patch_token False</div>
</div><!-- fragment --><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Base LLM   </th><th class="markdownTableHeadNone">Vision Encoder   </th><th class="markdownTableHeadNone">Projection   </th><th class="markdownTableHeadNone">Pretrain Data   </th><th class="markdownTableHeadNone">Pretraining schedule   </th><th class="markdownTableHeadNone">Download    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v1.5   </td><td class="markdownTableBodyNone">CLIP-L-336px   </td><td class="markdownTableBodyNone">MLP-2x   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5">projector</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Vicuna-7B-v1.5   </td><td class="markdownTableBodyNone">CLIP-L-336px   </td><td class="markdownTableBodyNone">MLP-2x   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5">projector</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaMA-2-13B-Chat   </td><td class="markdownTableBodyNone">CLIP-L-336px   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-336px-pretrain-llama-2-13b-chat">projector</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">LLaMA-2-7B-Chat   </td><td class="markdownTableBodyNone">CLIP-L-336px   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-336px-pretrain-llama-2-7b-chat">projector</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaMA-2-13B-Chat   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-pretrain-llama-2-13b-chat">projector</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">LLaMA-2-7B-Chat   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-pretrain-llama-2-7b-chat">projector</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v1.3   </td><td class="markdownTableBodyNone">CLIP-L-336px   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-336px-pretrain-vicuna-13b-v1.3">projector</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Vicuna-7B-v1.3   </td><td class="markdownTableBodyNone">CLIP-L-336px   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-336px-pretrain-vicuna-7b-v1.3">projector</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v1.3   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-pretrain-vicuna-13b-v1.3">projector</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Vicuna-7B-v1.3   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">Linear   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-pretrain-vicuna-7b-v1.3">projector</a>   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md50"></a>
Science QA Checkpoints</h1>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Base LLM   </th><th class="markdownTableHeadNone">Vision Encoder   </th><th class="markdownTableHeadNone">Pretrain Data   </th><th class="markdownTableHeadNone">Pretraining schedule   </th><th class="markdownTableHeadNone">Finetuning Data   </th><th class="markdownTableHeadNone">Finetuning schedule   </th><th class="markdownTableHeadNone">Download    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v1.3   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">ScienceQA   </td><td class="markdownTableBodyNone">full_ft-12e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3">ckpt</a>   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md51"></a>
Legacy Models (merged weights)</h1>
<p>The model weights below are <em>merged</em> weights. You do not need to apply delta. The usage of LLaVA checkpoints should comply with the base LLM's model license.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Base LLM   </th><th class="markdownTableHeadNone">Vision Encoder   </th><th class="markdownTableHeadNone">Pretrain Data   </th><th class="markdownTableHeadNone">Pretraining schedule   </th><th class="markdownTableHeadNone">Finetuning Data   </th><th class="markdownTableHeadNone">Finetuning schedule   </th><th class="markdownTableHeadNone">Download    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">MPT-7B-Chat   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-80K   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview">preview</a>   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md52"></a>
Legacy Models (delta weights)</h1>
<p>The model weights below are <em>delta</em> weights. The usage of LLaVA checkpoints should comply with the base LLM's model license: <a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md">LLaMA</a>.</p>
<p>You can add our delta to the original LLaMA weights to obtain the LLaVA weights.</p>
<p>Instructions:</p>
<ol type="1">
<li>Get the original LLaMA weights in the huggingface format by following the instructions <a href="https://huggingface.co/docs/transformers/main/model_doc/llama">here</a>.</li>
<li>Use the following scripts to get LLaVA weights by applying our delta. It will automatically download delta weights from our Hugging Face account. In the script below, we use the delta weights of <a href="https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0"><code>liuhaotian/LLaVA-7b-delta-v0</code></a> as an example. It can be adapted for other delta weights by changing the <code>--delta</code> argument (and base/target accordingly).</li>
</ol>
<div class="fragment"><div class="line">python3 -m llava.model.apply_delta \</div>
<div class="line">    --base /path/to/llama-7b \</div>
<div class="line">    --target /output/path/to/LLaVA-7B-v0 \</div>
<div class="line">    --delta liuhaotian/LLaVA-7b-delta-v0</div>
</div><!-- fragment --><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Base LLM   </th><th class="markdownTableHeadNone">Vision Encoder   </th><th class="markdownTableHeadNone">Pretrain Data   </th><th class="markdownTableHeadNone">Pretraining schedule   </th><th class="markdownTableHeadNone">Finetuning Data   </th><th class="markdownTableHeadNone">Finetuning schedule   </th><th class="markdownTableHeadNone">Download    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v1.1   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">CC-595K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-158K   </td><td class="markdownTableBodyNone">full_ft-3e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-13b-delta-v1-1">delta-weights</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Vicuna-7B-v1.1   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-80K   </td><td class="markdownTableBodyNone">full_ft-1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1">delta-weights</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v0   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">CC-595K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-158K   </td><td class="markdownTableBodyNone">full_ft-3e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0">delta-weights</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Vicuna-13B-v0   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">CC-595K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">ScienceQA   </td><td class="markdownTableBodyNone">full_ft-12e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0-science_qa">delta-weights</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-7B-v0   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">CC-595K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone">LLaVA-Instruct-158K   </td><td class="markdownTableBodyNone">full_ft-3e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0">delta-weights</a>   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md53"></a>
Legacy Projector weights</h1>
<p>The following projector weights are deprecated, and the support for them may be removed in the future. They do not support zero-shot inference. Please use the projector weights in the table above if possible.</p>
<p><b>NOTE</b>: When you use our pretrained projector for visual instruction tuning, it is very important to <b>use the same base LLM and vision encoder</b> as the one we used for pretraining the projector. Otherwise, the performance will be very bad.</p>
<p>When using these projector weights to instruction tune your LMM, please make sure that these options are correctly set as follows,</p>
<div class="fragment"><div class="line">--mm_use_im_start_end True</div>
<div class="line">--mm_use_im_patch_token False</div>
</div><!-- fragment --><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Base LLM   </th><th class="markdownTableHeadNone">Vision Encoder   </th><th class="markdownTableHeadNone">Pretrain Data   </th><th class="markdownTableHeadNone">Pretraining schedule   </th><th class="markdownTableHeadNone">Download    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-7B-v1.1   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">LCS-558K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-Pretrained-Projectors/blob/main/LLaVA-7b-pretrain-projector-v1-1-LCS-558K-blip_caption.bin">projector</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Vicuna-13B-v0   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">CC-595K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-Pretrained-Projectors/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption.bin">projector</a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-7B-v0   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">CC-595K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-Pretrained-Projectors/blob/main/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin">projector</a>   </td></tr>
</table>
<p>When using these projector weights to instruction tune your LMM, please make sure that these options are correctly set as follows,</p>
<div class="fragment"><div class="line">--mm_use_im_start_end False</div>
<div class="line">--mm_use_im_patch_token False</div>
</div><!-- fragment --><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Base LLM   </th><th class="markdownTableHeadNone">Vision Encoder   </th><th class="markdownTableHeadNone">Pretrain Data   </th><th class="markdownTableHeadNone">Pretraining schedule   </th><th class="markdownTableHeadNone">Download    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Vicuna-13B-v0   </td><td class="markdownTableBodyNone">CLIP-L   </td><td class="markdownTableBodyNone">CC-595K   </td><td class="markdownTableBodyNone">1e   </td><td class="markdownTableBodyNone"><a href="https://huggingface.co/liuhaotian/LLaVA-Pretrained-Projectors/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin">projector</a>   </td></tr>
</table>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.0 </li>
  </ul>
</div>
</body>
</html>
