<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>LLaVA: Evaluation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">LLaVA<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('md_docs_2_evaluation.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Evaluation</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md6"></a></p>
<p>In LLaVA-1.5, we evaluate models on a diverse set of 12 benchmarks. To ensure the reproducibility, we evaluate the models with greedy decoding. We do not evaluate using beam search to make the inference process consistent with the chat demo of real-time outputs.</p>
<p>Currently, we mostly utilize the official toolkit or server for the evaluation.</p>
<h1><a class="anchor" id="autotoc_md7"></a>
Evaluate on Custom Datasets</h1>
<p>You can evaluate LLaVA on your custom datasets by converting your dataset to LLaVA's jsonl format, and evaluate using <a href="https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa.py"><code>model_vqa.py</code></a>.</p>
<p>Below we provide a general guideline for evaluating datasets with some common formats.</p>
<ol type="1">
<li>Short-answer (e.g. VQAv2, MME).</li>
</ol>
<div class="fragment"><div class="line">&lt;question&gt;</div>
<div class="line">Answer the question using a single word or phrase.</div>
</div><!-- fragment --><ol type="1">
<li>Option-only for multiple-choice (e.g. MMBench, SEED-Bench).</li>
</ol>
<div class="fragment"><div class="line">&lt;question&gt;</div>
<div class="line">A. &lt;option_1&gt;</div>
<div class="line">B. &lt;option_2&gt;</div>
<div class="line">C. &lt;option_3&gt;</div>
<div class="line">D. &lt;option_4&gt;</div>
<div class="line">Answer with the option&#39;s letter from the given choices directly.</div>
</div><!-- fragment --><ol type="1">
<li>Natural QA (e.g. LLaVA-Bench, MM-Vet).</li>
</ol>
<p>No postprocessing is needed.</p>
<h1><a class="anchor" id="autotoc_md8"></a>
Scripts</h1>
<p>Before preparing task-specific data, <b>you MUST first download <a href="https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing">eval.zip</a></b>. It contains custom annotations, scripts, and the prediction files with LLaVA v1.5. Extract to <code>./playground/data/eval</code>. This also provides a general structure for all datasets.</p>
<h2><a class="anchor" id="autotoc_md9"></a>
VQAv2</h2>
<ol type="1">
<li>Download <a href="http://images.cocodataset.org/zips/test2015.zip"><code>test2015</code></a> and put it under <code>./playground/data/eval/vqav2</code>.</li>
<li>Multi-GPU inference. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/eval/vqav2.sh</div>
</div><!-- fragment --></li>
<li>Submit the results to the <a href="https://eval.ai/web/challenges/challenge-page/830/my-submission">evaluation server</a>: <code>./playground/data/eval/vqav2/answers_upload</code>.</li>
</ol>
<h2><a class="anchor" id="autotoc_md10"></a>
GQA</h2>
<ol type="1">
<li>Download the <a href="https://cs.stanford.edu/people/dorarad/gqa/download.html">data</a> and <a href="https://cs.stanford.edu/people/dorarad/gqa/evaluate.html">evaluation scripts</a> following the official instructions and put under <code>./playground/data/eval/gqa/data</code>. You may need to modify <code>eval.py</code> as <a href="https://gist.github.com/haotian-liu/db6eddc2a984b4cbcc8a7f26fd523187">this</a> due to the missing assets in the GQA v1.2 release.</li>
<li>Multi-GPU inference. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/eval/gqa.sh</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md11"></a>
VisWiz</h2>
<ol type="1">
<li>Download <a href="https://vizwiz.cs.colorado.edu/VizWiz_final/vqa_data/Annotations.zip"><code>test.json</code></a> and extract <a href="https://vizwiz.cs.colorado.edu/VizWiz_final/images/test.zip"><code>test.zip</code></a> to <code>test</code>. Put them under <code>./playground/data/eval/vizwiz</code>.</li>
<li>Single-GPU inference. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/vizwiz.sh</div>
</div><!-- fragment --></li>
<li>Submit the results to the <a href="https://eval.ai/web/challenges/challenge-page/2185/my-submission">evaluation server</a>: <code>./playground/data/eval/vizwiz/answers_upload</code>.</li>
</ol>
<h2><a class="anchor" id="autotoc_md12"></a>
ScienceQA</h2>
<ol type="1">
<li>Under <code>./playground/data/eval/scienceqa</code>, download <code>images</code>, <code>pid_splits.json</code>, <code>problems.json</code> from the <code>data/scienceqa</code> folder of the ScienceQA <a href="https://github.com/lupantech/ScienceQA">repo</a>.</li>
<li>Single-GPU inference and evaluate. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/sqa.sh</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md13"></a>
TextVQA</h2>
<ol type="1">
<li>Download <a href="https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_val.json"><code>TextVQA_0.5.1_val.json</code></a> and <a href="https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip">images</a> and extract to <code>./playground/data/eval/textvqa</code>.</li>
<li>Single-GPU inference and evaluate. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/textvqa.sh</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md14"></a>
POPE</h2>
<ol type="1">
<li>Download <code>coco</code> from <a href="https://github.com/AoiDragon/POPE/tree/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco">POPE</a> and put under <code>./playground/data/eval/pope</code>.</li>
<li>Single-GPU inference and evaluate. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/pope.sh</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md15"></a>
MME</h2>
<ol type="1">
<li>Download the data following the official instructions <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">here</a>.</li>
<li>Downloaded images to <code>MME_Benchmark_release_version</code>.</li>
<li>put the official <code>eval_tool</code> and <code>MME_Benchmark_release_version</code> under <code>./playground/data/eval/MME</code>.</li>
<li>Single-GPU inference and evaluate. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/mme.sh</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md16"></a>
MMBench</h2>
<ol type="1">
<li>Download <a href="https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_20230712.tsv"><code>mmbench_dev_20230712.tsv</code></a> and put under <code>./playground/data/eval/mmbench</code>.</li>
<li>Single-GPU inference. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/mmbench.sh</div>
</div><!-- fragment --></li>
<li>Submit the results to the <a href="https://opencompass.org.cn/leaderboard-multimodal">evaluation server</a>: <code>./playground/data/eval/mmbench/answers_upload/mmbench_dev_20230712</code>.</li>
</ol>
<h2><a class="anchor" id="autotoc_md17"></a>
MMBench-CN</h2>
<ol type="1">
<li>Download <a href="https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_cn_20231003.tsv"><code>mmbench_dev_cn_20231003.tsv</code></a> and put under <code>./playground/data/eval/mmbench</code>.</li>
<li>Single-GPU inference. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/mmbench_cn.sh</div>
</div><!-- fragment --></li>
<li>Submit the results to the evaluation server: <code>./playground/data/eval/mmbench/answers_upload/mmbench_dev_cn_20231003</code>.</li>
</ol>
<h2><a class="anchor" id="autotoc_md18"></a>
SEED-Bench</h2>
<ol type="1">
<li>Following the official <a href="https://github.com/AILab-CVC/SEED-Bench/blob/main/DATASET.md">instructions</a> to download the images and the videos. Put images under <code>./playground/data/eval/seed_bench/SEED-Bench-image</code>.</li>
<li>Extract the video frame in the middle from the downloaded videos, and put them under <code>./playground/data/eval/seed_bench/SEED-Bench-video-image</code>. We provide our script <code>extract_video_frames.py</code> modified from the official one.</li>
<li>Multiple-GPU inference and evaluate. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/eval/seed.sh</div>
</div><!-- fragment --></li>
<li>Optionally, submit the results to the leaderboard: <code>./playground/data/eval/seed_bench/answers_upload</code> using the official jupyter notebook.</li>
</ol>
<h2><a class="anchor" id="autotoc_md19"></a>
LLaVA-Bench-in-the-Wild</h2>
<ol type="1">
<li>Extract contents of <a href="https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild"><code>llava-bench-in-the-wild</code></a> to <code>./playground/data/eval/llava-bench-in-the-wild</code>.</li>
<li>Single-GPU inference and evaluate. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/llavabench.sh</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md20"></a>
MM-Vet</h2>
<ol type="1">
<li>Extract <a href="https://github.com/yuweihao/MM-Vet/releases/download/v1/mm-vet.zip"><code>mm-vet.zip</code></a> to <code>./playground/data/eval/mmvet</code>.</li>
<li>Single-GPU inference. <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/mmvet.sh</div>
</div><!-- fragment --></li>
<li>Evaluate the predictions in <code>./playground/data/eval/mmvet/results</code> using the official jupyter notebook.</li>
</ol>
<h1><a class="anchor" id="autotoc_md21"></a>
More Benchmarks</h1>
<p>Below are awesome benchmarks for multimodal understanding from the research community, that are not initially included in the LLaVA-1.5 release.</p>
<h2><a class="anchor" id="autotoc_md22"></a>
Q-Bench</h2>
<ol type="1">
<li>Download <a href="https://huggingface.co/datasets/nanyangtu/LLVisionQA-QBench/resolve/main/llvisionqa_dev.json"><code>llvisionqa_dev.json</code></a> (for <code>dev</code>-subset) and <a href="https://huggingface.co/datasets/nanyangtu/LLVisionQA-QBench/resolve/main/llvisionqa_test.json"><code>llvisionqa_test.json</code></a> (for <code>test</code>-subset). Put them under <code>./playground/data/eval/qbench</code>.</li>
<li>Download and extract <a href="https://huggingface.co/datasets/nanyangtu/LLVisionQA-QBench/resolve/main/images_llvisionqa.tar">images</a> and put all the images directly under <code>./playground/data/eval/qbench/images_llviqionqa</code>.</li>
<li>Single-GPU inference (change <code>dev</code> to <code>test</code> for evaluation on test set). <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/qbench.sh dev</div>
</div><!-- fragment --></li>
<li>Submit the results by instruction <a href="https://github.com/VQAssessment/Q-Bench#option-1-submit-results">here</a>: <code>./playground/data/eval/qbench/llvisionqa_dev_answers.jsonl</code>.</li>
</ol>
<h2><a class="anchor" id="autotoc_md23"></a>
Chinese-Q-Bench</h2>
<ol type="1">
<li>Download <a href="https://huggingface.co/datasets/nanyangtu/LLVisionQA-QBench/resolve/main/%E8%B4%A8%E8%A1%A1-%E9%97%AE%E7%AD%94-%E9%AA%8C%E8%AF%81%E9%9B%86.json"><code>质衡-问答-验证集.json</code></a> (for <code>dev</code>-subset) and <a href="https://huggingface.co/datasets/nanyangtu/LLVisionQA-QBench/resolve/main/%E8%B4%A8%E8%A1%A1-%E9%97%AE%E7%AD%94-%E6%B5%8B%E8%AF%95%E9%9B%86.json"><code>质衡-问答-测试集.json</code></a> (for <code>test</code>-subset). Put them under <code>./playground/data/eval/qbench</code>.</li>
<li>Download and extract <a href="https://huggingface.co/datasets/nanyangtu/LLVisionQA-QBench/resolve/main/images_llvisionqa.tar">images</a> and put all the images directly under <code>./playground/data/eval/qbench/images_llviqionqa</code>.</li>
<li>Single-GPU inference (change <code>dev</code> to <code>test</code> for evaluation on test set). <div class="fragment"><div class="line">CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/qbench_zh.sh dev</div>
</div><!-- fragment --></li>
<li>Submit the results by instruction <a href="https://github.com/VQAssessment/Q-Bench#option-1-submit-results">here</a>: <code>./playground/data/eval/qbench/llvisionqa_zh_dev_answers.jsonl</code>. </li>
</ol>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.0 </li>
  </ul>
</div>
</body>
</html>
