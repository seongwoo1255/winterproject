\chapter{LLa\+VA-\/\+Bench \mbox{[}\texorpdfstring{$<$}{<}a href="{}https\+://huggingface.\+co/datasets/liuhaotian/llava-\/bench-\/in-\/the-\/wild"{} \texorpdfstring{$>$}{>}Download\texorpdfstring{$<$}{<}/a\texorpdfstring{$>$}{>}\mbox{]}}
\hypertarget{md_docs_2_l_la_v_a___bench}{}\label{md_docs_2_l_la_v_a___bench}\index{LLaVA-\/Bench \mbox{[}$<$a href="{}https://huggingface.co/datasets/liuhaotian/llava-\/bench-\/in-\/the-\/wild"{} $>$Download$<$/a$>$\mbox{]}@{LLaVA-\/Bench [$<$a href=""https://huggingface.co/datasets/liuhaotian/llava-\/bench-\/in-\/the-\/wild"" $>$Download$<$/a$>$]}}
\label{md_docs_2_l_la_v_a___bench_autotoc_md28}%
\Hypertarget{md_docs_2_l_la_v_a___bench_autotoc_md28}%


{\bfseries{-\/Introduction-\/}} Large commercial multimodal chatbots have been released in this week, including
\begin{DoxyItemize}
\item \href{https://blogs.bing.com/search/july-2023/Bing-Chat-Enterprise-announced,-multimodal-Visual-Search-rolling-out-to-Bing-Chat}{\texttt{ Multimodal Bing-\/\+Chat by Microsoft}} (July 18, 2023)
\item \href{https://bard.google.com/}{\texttt{ Multimodal Bard by Google}}.
\end{DoxyItemize}

These chatbots are presumably supported by proprietary large multimodal models (LMM). Compared with the open-\/source LMM such as LLa\+VA, proprietary LMM represent the scaling success upperbound of the current So\+TA techniques. They share the goal of developing multimodal chatbots that follow human intents to complete various daily-\/life visual tasks in the wild. While it remains less explored how to evaluate multimodal chat ability, it provides useful feedback to study open-\/source LMMs against the commercial multimodal chatbots. In addition to the {\itshape LLa\+VA-\/\+Bench (COCO)} dataset we used to develop the early versions of LLa\+VA, we are releasing \href{https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild}{\texttt{ {\itshape LLa\+VA-\/\+Bench (In-\/the-\/\+Wild)}}} to the community for the public use.\hypertarget{md_docs_2_l_la_v_a___bench_autotoc_md29}{}\doxysection{\texorpdfstring{LLa\+VA-\/\+Bench (In-\/the-\/\+Wild {\itshape \mbox{[}Ongoing work\mbox{]}})}{LLa\+VA-\/\+Bench (In-\/the-\/\+Wild {\itshape \mbox{[}Ongoing work\mbox{]}})}}\label{md_docs_2_l_la_v_a___bench_autotoc_md29}
To evaluate the model\textquotesingle{}s capability in more challenging tasks and generalizability to novel domains, we collect a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc, and associate each image with a highly-\/detailed and manually-\/curated description and a proper selection of questions. Such design also assesses the model\textquotesingle{}s robustness to different prompts. In this release, we also categorize questions into three categories\+: conversation (simple QA), detailed description, and complex reasoning. We continue to expand and improve the diversity of the LLa\+VA-\/\+Bench (In-\/the-\/\+Wild). We manually query Bing-\/\+Chat and Bard to get the responses.\hypertarget{md_docs_2_l_la_v_a___bench_autotoc_md30}{}\doxysubsection{\texorpdfstring{Results}{Results}}\label{md_docs_2_l_la_v_a___bench_autotoc_md30}
The score is measured by comparing against a reference answer generated by text-\/only GPT-\/4. It is generated by feeding the question, along with the ground truth image annotations as the context. A text-\/only GPT-\/4 evaluator rates both answers. We query GPT-\/4 by putting the reference answer first, and then the answer generated by the candidate model. We upload images at their original resolution to Bard and Bing-\/\+Chat to obtain the results.

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Approach   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Conversation   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Detail   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Reasoning   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Overall    }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Approach   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Conversation   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Detail   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Reasoning   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Overall    }\\\cline{1-5}
\endhead
Bard-\/0718   &83.\+7   &69.\+7   &78.\+7   &77.\+8    \\\cline{1-5}
Bing-\/\+Chat-\/0629   &59.\+6   &52.\+2   &90.\+1   &71.\+5    \\\cline{1-5}
LLa\+VA-\/13\+B-\/v1-\/336px-\/0719 (beam=1)   &64.\+3   &55.\+9   &81.\+7   &70.\+1    \\\cline{1-5}
LLa\+VA-\/13\+B-\/v1-\/336px-\/0719 (beam=5)   &68.\+4   &59.\+9   &84.\+3   &73.\+5   \\\cline{1-5}
\end{longtabu}


Note that Bard sometimes refuses to answer questions about images containing humans, and Bing-\/\+Chat blurs the human faces in the images. We also provide the benchmark score for the subset without humans.

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Approach   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Conversation   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Detail   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Reasoning   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Overall    }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Approach   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Conversation   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Detail   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Reasoning   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Overall    }\\\cline{1-5}
\endhead
Bard-\/0718   &94.\+9   &74.\+3   &84.\+3   &84.\+6    \\\cline{1-5}
Bing-\/\+Chat-\/0629   &55.\+8   &53.\+6   &93.\+5   &72.\+6    \\\cline{1-5}
LLa\+VA-\/13\+B-\/v1-\/336px-\/0719 (beam=1)   &62.\+2   &56.\+4   &82.\+2   &70.\+0    \\\cline{1-5}
LLa\+VA-\/13\+B-\/v1-\/336px-\/0719 (beam=5)   &65.\+6   &61.\+7   &85.\+0   &73.\+6   \\\cline{1-5}
\end{longtabu}
