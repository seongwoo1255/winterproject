\chapter{LLa\+VA (Lo\+RA, Preview)}
\hypertarget{md_docs_2_lo_r_a}{}\label{md_docs_2_lo_r_a}\index{LLaVA (LoRA, Preview)@{LLaVA (LoRA, Preview)}}
\label{md_docs_2_lo_r_a_autotoc_md35}%
\Hypertarget{md_docs_2_lo_r_a_autotoc_md35}%


NOTE\+: This is a technical preview, and is not yet ready for production use. We are still running hyperparameter search for the Lo\+RA model, and will release the final model soon. If you\textquotesingle{}d like to contribute to this, please contact us.

You need latest code base for Lo\+RA support (instructions \href{https://github.com/haotian-liu/LLaVA\#upgrade-to-latest-code-base}{\texttt{ here}})\hypertarget{md_docs_2_lo_r_a_autotoc_md36}{}\doxysection{\texorpdfstring{Demo (Web UI)}{Demo (Web UI)}}\label{md_docs_2_lo_r_a_autotoc_md36}
Please execute each of the commands below one by one (after the previous one has finished). The commands are the same as launching other demos except for an additional {\ttfamily -\/-\/model-\/base} flag to specify the base model to use. Please make sure the base model corresponds to the Lo\+RA checkpoint that you are using. For this technical preview, you need Vicuna v1.\+1 (7B) checkpoint (if you do not have that already, follow the instructions \href{https://github.com/lm-sys/FastChat\#vicuna-weights}{\texttt{ here}}).\hypertarget{md_docs_2_lo_r_a_autotoc_md37}{}\doxysubsubsection{\texorpdfstring{Launch a controller}{Launch a controller}}\label{md_docs_2_lo_r_a_autotoc_md37}

\begin{DoxyCode}{0}
\DoxyCodeLine{python\ -\/m\ llava.serve.controller\ -\/-\/host\ 0.0.0.0\ -\/-\/port\ 10000}

\end{DoxyCode}
\hypertarget{md_docs_2_lo_r_a_autotoc_md38}{}\doxysubsubsection{\texorpdfstring{Launch a gradio web server.}{Launch a gradio web server.}}\label{md_docs_2_lo_r_a_autotoc_md38}

\begin{DoxyCode}{0}
\DoxyCodeLine{python\ -\/m\ llava.serve.gradio\_web\_server\ -\/-\/controller\ http://localhost:10000\ -\/-\/model-\/list-\/mode\ reload}

\end{DoxyCode}
 You just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.\hypertarget{md_docs_2_lo_r_a_autotoc_md39}{}\doxysubsubsection{\texorpdfstring{Launch a model worker}{Launch a model worker}}\label{md_docs_2_lo_r_a_autotoc_md39}

\begin{DoxyCode}{0}
\DoxyCodeLine{python\ -\/m\ llava.serve.model\_worker\ -\/-\/host\ 0.0.0.0\ -\/-\/controller\ http://localhost:10000\ -\/-\/port\ 40000\ -\/-\/worker\ http://localhost:40000\ -\/-\/model-\/path\ liuhaotian/llava-\/vicuna-\/7b-\/v1.1-\/lcs\_558k-\/instruct\_80k\_3e-\/lora-\/preview-\/alpha\ -\/-\/model-\/base\ /path/to/vicuna-\/v1.1}

\end{DoxyCode}
 Wait until the process finishes loading the model and you see "{}\+Uvicorn running on ..."{}. Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.

You can launch as many workers as you want, and compare between different model checkpoints in the same Gradio interface. Please keep the {\ttfamily -\/-\/controller} the same, and modify the {\ttfamily -\/-\/port} and {\ttfamily -\/-\/worker} to a different port number for each worker.\hypertarget{md_docs_2_lo_r_a_autotoc_md40}{}\doxysection{\texorpdfstring{Training}{Training}}\label{md_docs_2_lo_r_a_autotoc_md40}
Please see sample training scripts for \href{https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_lora.sh}{\texttt{ Lo\+RA}} and \href{https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_qlora.sh}{\texttt{ QLo\+RA}}.

We provide sample Deep\+Speed configs, \href{https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3.json}{\texttt{ {\ttfamily zero3.\+json}}} is more like Py\+Torch FSDP, and \href{https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3_offload.json}{\texttt{ {\ttfamily zero3\+\_\+offload.\+json}}} can further save memory consumption by offloading parameters to CPU. {\ttfamily zero3.\+json} is usually faster than {\ttfamily zero3\+\_\+offload.\+json} but requires more GPU memory, therefore, we recommend trying {\ttfamily zero3.\+json} first, and if you run out of GPU memory, try {\ttfamily zero3\+\_\+offload.\+json}. You can also tweak the {\ttfamily per\+\_\+device\+\_\+train\+\_\+batch\+\_\+size} and {\ttfamily gradient\+\_\+accumulation\+\_\+steps} in the config to save memory, and just to make sure that {\ttfamily per\+\_\+device\+\_\+train\+\_\+batch\+\_\+size} and {\ttfamily gradient\+\_\+accumulation\+\_\+steps} remains the same.

If you are having issues with Ze\+RO-\/3 configs, and there are enough VRAM, you may try \href{https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero2.json}{\texttt{ {\ttfamily zero2.\+json}}}. This consumes slightly more memory than Ze\+RO-\/3, and behaves more similar to Py\+Torch FSDP, while still supporting parameter-\/efficient tuning.\hypertarget{md_docs_2_lo_r_a_autotoc_md41}{}\doxysection{\texorpdfstring{Create Merged Checkpoints}{Create Merged Checkpoints}}\label{md_docs_2_lo_r_a_autotoc_md41}

\begin{DoxyCode}{0}
\DoxyCodeLine{python\ scripts/merge\_lora\_weights.py\ \(\backslash\)}
\DoxyCodeLine{\ \ \ \ -\/-\/model-\/path\ /path/to/lora\_model\ \(\backslash\)}
\DoxyCodeLine{\ \ \ \ -\/-\/model-\/base\ /path/to/base\_model\ \(\backslash\)}
\DoxyCodeLine{\ \ \ \ -\/-\/save-\/model-\/path\ /path/to/merge\_model}

\end{DoxyCode}
 