\doxysection{llava/train/llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.py File Reference}
\hypertarget{llama__flash__attn__monkey__patch_8py}{}\label{llama__flash__attn__monkey__patch_8py}\index{llava/train/llama\_flash\_attn\_monkey\_patch.py@{llava/train/llama\_flash\_attn\_monkey\_patch.py}}
\doxysubsubsection*{Namespaces}
\begin{DoxyCompactItemize}
\item 
namespace \mbox{\hyperlink{namespacellama__flash__attn__monkey__patch}{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch}}
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
Tuple\mbox{[}torch.\+Tensor, Optional\mbox{[}torch.\+Tensor\mbox{]}, Optional\mbox{[}Tuple\mbox{[}torch.\+Tensor\mbox{]}\mbox{]}\mbox{]} \mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a5c68ef61a0287d74e55e329f382ad090}{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+forward}} (self, torch.\+Tensor hidden\+\_\+states, Optional\mbox{[}torch.\+Tensor\mbox{]} attention\+\_\+mask=None, Optional\mbox{[}torch.\+Tensor\mbox{]} position\+\_\+ids=None, Optional\mbox{[}Tuple\mbox{[}torch.\+Tensor\mbox{]}\mbox{]} past\+\_\+key\+\_\+value=None, bool output\+\_\+attentions=False, bool use\+\_\+cache=False)
\item 
\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7070583aef8b7fa93fe85f6792b5ee9d}{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+\_\+prepare\+\_\+decoder\+\_\+attention\+\_\+mask}} (self, attention\+\_\+mask, input\+\_\+shape, inputs\+\_\+embeds, past\+\_\+key\+\_\+values\+\_\+length)
\item 
\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7593dec7e8a06447dd33e4514015a0a4}{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+replace\+\_\+llama\+\_\+attn\+\_\+with\+\_\+flash\+\_\+attn}} ()
\end{DoxyCompactItemize}
