\doxysection{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+py}
\hypertarget{llama__flash__attn__monkey__patch_8py_source}{}\label{llama__flash__attn__monkey__patch_8py_source}\index{llava/train/llama\_flash\_attn\_monkey\_patch.py@{llava/train/llama\_flash\_attn\_monkey\_patch.py}}
\mbox{\hyperlink{llama__flash__attn__monkey__patch_8py}{Go to the documentation of this file.}}
\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00001}\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch}{00001}}\ \textcolor{keyword}{from}\ typing\ \textcolor{keyword}{import}\ Optional,\ Tuple}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00002}00002\ \textcolor{keyword}{import}\ warnings}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00003}00003\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00004}00004\ \textcolor{keyword}{import}\ torch}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00005}00005\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00006}00006\ \textcolor{keyword}{import}\ transformers}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00007}00007\ \textcolor{keyword}{from}\ transformers.models.llama.modeling\_llama\ \textcolor{keyword}{import}\ apply\_rotary\_pos\_emb,\ repeat\_kv}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00008}00008\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00009}00009\ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00010}00010\ \ \ \ \ \textcolor{keyword}{from}\ flash\_attn.flash\_attn\_interface\ \textcolor{keyword}{import}\ flash\_attn\_unpadded\_qkvpacked\_func}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00011}00011\ \textcolor{keywordflow}{except}\ ImportError:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00012}00012\ \ \ \ \ \textcolor{keyword}{from}\ flash\_attn.flash\_attn\_interface\ \textcolor{keyword}{import}\ flash\_attn\_varlen\_qkvpacked\_func\ \textcolor{keyword}{as}\ flash\_attn\_unpadded\_qkvpacked\_func}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00013}00013\ \textcolor{keyword}{from}\ flash\_attn.bert\_padding\ \textcolor{keyword}{import}\ unpad\_input,\ pad\_input}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00014}00014\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00015}00015\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00016}\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a5c68ef61a0287d74e55e329f382ad090}{00016}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a5c68ef61a0287d74e55e329f382ad090}{forward}}(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00017}00017\ \ \ \ \ self,}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00018}00018\ \ \ \ \ hidden\_states:\ torch.Tensor,}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00019}00019\ \ \ \ \ attention\_mask:\ Optional[torch.Tensor]\ =\ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00020}00020\ \ \ \ \ position\_ids:\ Optional[torch.Tensor]\ =\ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00021}00021\ \ \ \ \ past\_key\_value:\ Optional[Tuple[torch.Tensor]]\ =\ \textcolor{keywordtype}{None},}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00022}00022\ \ \ \ \ output\_attentions:\ bool\ =\ \textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00023}00023\ \ \ \ \ use\_cache:\ bool\ =\ \textcolor{keyword}{False},}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00024}00024\ )\ -\/>\ Tuple[torch.Tensor,\ Optional[torch.Tensor],\ Optional[Tuple[torch.Tensor]]]:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00025}00025\ \ \ \ \ \textcolor{keywordflow}{if}\ output\_attentions:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00026}00026\ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00027}00027\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Output\ attentions\ is\ not\ supported\ for\ patched\ \`{}LlamaAttention`,\ returning\ \`{}None`\ instead."{}}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00028}00028\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00029}00029\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00030}00030\ \ \ \ \ bsz,\ q\_len,\ \_\ =\ hidden\_states.size()}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00031}00031\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00032}00032\ \ \ \ \ query\_states\ =\ (}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00033}00033\ \ \ \ \ \ \ \ \ self.q\_proj(hidden\_states)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00034}00034\ \ \ \ \ \ \ \ \ .view(bsz,\ q\_len,\ self.num\_heads,\ self.head\_dim)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00035}00035\ \ \ \ \ \ \ \ \ .transpose(1,\ 2)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00036}00036\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00037}00037\ \ \ \ \ key\_states\ =\ (}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00038}00038\ \ \ \ \ \ \ \ \ self.k\_proj(hidden\_states)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00039}00039\ \ \ \ \ \ \ \ \ .view(bsz,\ q\_len,\ self.num\_key\_value\_heads,\ self.head\_dim)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00040}00040\ \ \ \ \ \ \ \ \ .transpose(1,\ 2)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00041}00041\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00042}00042\ \ \ \ \ value\_states\ =\ (}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00043}00043\ \ \ \ \ \ \ \ \ self.v\_proj(hidden\_states)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00044}00044\ \ \ \ \ \ \ \ \ .view(bsz,\ q\_len,\ self.num\_key\_value\_heads,\ self.head\_dim)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00045}00045\ \ \ \ \ \ \ \ \ .transpose(1,\ 2)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00046}00046\ \ \ \ \ )\ \ \textcolor{comment}{\#\ shape:\ (b,\ num\_heads,\ s,\ head\_dim)}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00047}00047\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00048}00048\ \ \ \ \ kv\_seq\_len\ =\ key\_states.shape[-\/2]}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00049}00049\ \ \ \ \ \textcolor{keywordflow}{if}\ past\_key\_value\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00050}00050\ \ \ \ \ \ \ \ \ kv\_seq\_len\ +=\ past\_key\_value[0].shape[-\/2]}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00051}00051\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00052}00052\ \ \ \ \ cos,\ sin\ =\ self.rotary\_emb(value\_states,\ seq\_len=kv\_seq\_len)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00053}00053\ \ \ \ \ query\_states,\ key\_states\ =\ apply\_rotary\_pos\_emb(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00054}00054\ \ \ \ \ \ \ \ \ query\_states,\ key\_states,\ cos,\ sin,\ position\_ids}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00055}00055\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00056}00056\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00057}00057\ \ \ \ \ \textcolor{keywordflow}{if}\ past\_key\_value\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00058}00058\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ reuse\ k,\ v}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00059}00059\ \ \ \ \ \ \ \ \ key\_states\ =\ torch.cat([past\_key\_value[0],\ key\_states],\ dim=2)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00060}00060\ \ \ \ \ \ \ \ \ value\_states\ =\ torch.cat([past\_key\_value[1],\ value\_states],\ dim=2)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00061}00061\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00062}00062\ \ \ \ \ past\_key\_value\ =\ (key\_states,\ value\_states)\ \textcolor{keywordflow}{if}\ use\_cache\ \textcolor{keywordflow}{else}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00063}00063\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00064}00064\ \ \ \ \ \textcolor{comment}{\#\ repeat\ k/v\ heads\ if\ n\_kv\_heads\ <\ n\_heads}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00065}00065\ \ \ \ \ key\_states\ =\ repeat\_kv(key\_states,\ self.num\_key\_value\_groups)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00066}00066\ \ \ \ \ value\_states\ =\ repeat\_kv(value\_states,\ self.num\_key\_value\_groups)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00067}00067\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00068}00068\ \ \ \ \ \textcolor{comment}{\#\ Transform\ the\ data\ into\ the\ format\ required\ by\ flash\ attention}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00069}00069\ \ \ \ \ qkv\ =\ torch.stack([query\_states,\ key\_states,\ value\_states],\ dim=2)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00070}00070\ \ \ \ \ qkv\ =\ qkv.transpose(1,\ 3)\ \ \textcolor{comment}{\#\ shape:\ [b,\ s,\ 3,\ num\_heads,\ head\_dim]}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00071}00071\ \ \ \ \ key\_padding\_mask\ =\ attention\_mask}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00072}00072\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00073}00073\ \ \ \ \ \textcolor{keywordflow}{if}\ key\_padding\_mask\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00074}00074\ \ \ \ \ \ \ \ \ qkv\ =\ qkv.reshape(-\/1,\ 3,\ self.num\_heads,\ self.head\_dim)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ cu\_q\_lens\ =\ torch.arange(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00076}00076\ \ \ \ \ \ \ \ \ \ \ \ \ 0,\ (bsz\ +\ 1)\ *\ q\_len,\ step=q\_len,\ dtype=torch.int32,\ device=qkv.device}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00077}00077\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00078}00078\ \ \ \ \ \ \ \ \ max\_s\ =\ q\_len}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00079}00079\ \ \ \ \ \ \ \ \ output\ =\ flash\_attn\_unpadded\_qkvpacked\_func(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00080}00080\ \ \ \ \ \ \ \ \ \ \ \ \ qkv,\ cu\_q\_lens,\ max\_s,\ 0.0,\ softmax\_scale=\textcolor{keywordtype}{None},\ causal=\textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00081}00081\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00082}00082\ \ \ \ \ \ \ \ \ output\ =\ output.view(bsz,\ q\_len,\ -\/1)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00083}00083\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00084}00084\ \ \ \ \ \ \ \ \ qkv\ =\ qkv.reshape(bsz,\ q\_len,\ -\/1)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00085}00085\ \ \ \ \ \ \ \ \ qkv,\ indices,\ cu\_q\_lens,\ max\_s\ =\ unpad\_input(qkv,\ key\_padding\_mask)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00086}00086\ \ \ \ \ \ \ \ \ qkv\ =\ qkv.view(-\/1,\ 3,\ self.num\_heads,\ self.head\_dim)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00087}00087\ \ \ \ \ \ \ \ \ output\_unpad\ =\ flash\_attn\_unpadded\_qkvpacked\_func(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00088}00088\ \ \ \ \ \ \ \ \ \ \ \ \ qkv,\ cu\_q\_lens,\ max\_s,\ 0.0,\ softmax\_scale=\textcolor{keywordtype}{None},\ causal=\textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00089}00089\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00090}00090\ \ \ \ \ \ \ \ \ output\_unpad\ =\ output\_unpad.reshape(-\/1,\ self.num\_heads\ *\ self.head\_dim)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00091}00091\ \ \ \ \ \ \ \ \ output\ =\ pad\_input(output\_unpad,\ indices,\ bsz,\ q\_len)}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00092}00092\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00093}00093\ \ \ \ \ \textcolor{keywordflow}{return}\ self.o\_proj(output),\ \textcolor{keywordtype}{None},\ past\_key\_value}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00094}00094\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00095}00095\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00096}00096\ \textcolor{comment}{\#\ Disable\ the\ transformation\ of\ the\ attention\ mask\ in\ LlamaModel\ as\ the\ flash\ attention}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00097}00097\ \textcolor{comment}{\#\ requires\ the\ attention\ mask\ to\ be\ the\ same\ as\ the\ key\_padding\_mask}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00098}\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7070583aef8b7fa93fe85f6792b5ee9d}{00098}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7070583aef8b7fa93fe85f6792b5ee9d}{\_prepare\_decoder\_attention\_mask}}(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00099}00099\ \ \ \ \ self,\ attention\_mask,\ input\_shape,\ inputs\_embeds,\ past\_key\_values\_length}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00100}00100\ ):}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00101}00101\ \ \ \ \ \textcolor{comment}{\#\ [bsz,\ seq\_len]}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00102}00102\ \ \ \ \ \textcolor{keywordflow}{return}\ attention\_mask}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00103}00103\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00104}00104\ }
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00105}\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7593dec7e8a06447dd33e4514015a0a4}{00105}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7593dec7e8a06447dd33e4514015a0a4}{replace\_llama\_attn\_with\_flash\_attn}}():}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00106}00106\ \ \ \ \ cuda\_major,\ cuda\_minor\ =\ torch.cuda.get\_device\_capability()}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00107}00107\ \ \ \ \ \textcolor{keywordflow}{if}\ cuda\_major\ <\ 8:}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00108}00108\ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00109}00109\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Flash\ attention\ is\ only\ supported\ on\ A100\ or\ H100\ GPU\ during\ training\ due\ to\ head\ dim\ >\ 64\ backward."{}}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00110}00110\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}ref:\ https://github.com/HazyResearch/flash-\/attention/issues/190\#issuecomment-\/1523359593"{}}}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00111}00111\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00112}00112\ \ \ \ \ transformers.models.llama.modeling\_llama.LlamaModel.\_prepare\_decoder\_attention\_mask\ =\ (}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00113}00113\ \ \ \ \ \ \ \ \ \_prepare\_decoder\_attention\_mask}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00114}00114\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{llama__flash__attn__monkey__patch_8py_source_l00115}00115\ \ \ \ \ transformers.models.llama.modeling\_llama.LlamaAttention.forward\ =\ forward}

\end{DoxyCode}
