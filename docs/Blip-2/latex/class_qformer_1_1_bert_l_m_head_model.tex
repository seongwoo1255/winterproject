\doxysection{Qformer.\+Bert\+LMHead\+Model Class Reference}
\hypertarget{class_qformer_1_1_bert_l_m_head_model}{}\label{class_qformer_1_1_bert_l_m_head_model}\index{Qformer.BertLMHeadModel@{Qformer.BertLMHeadModel}}
Inheritance diagram for Qformer.\+Bert\+LMHead\+Model\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=3.000000cm]{class_qformer_1_1_bert_l_m_head_model}
\end{center}
\end{figure}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_a47b9e9493b149fdc6fa48ed3c7c01884}\label{class_qformer_1_1_bert_l_m_head_model_a47b9e9493b149fdc6fa48ed3c7c01884} 
{\bfseries \+\_\+\+\_\+init\+\_\+\+\_\+} (self, config)
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_ad8b557eef7c722dedbbf5e12d91875cc}\label{class_qformer_1_1_bert_l_m_head_model_ad8b557eef7c722dedbbf5e12d91875cc} 
{\bfseries get\+\_\+output\+\_\+embeddings} (self)
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_a28358efc7fbc1c198d5967090623f59c}\label{class_qformer_1_1_bert_l_m_head_model_a28358efc7fbc1c198d5967090623f59c} 
{\bfseries set\+\_\+output\+\_\+embeddings} (self, new\+\_\+embeddings)
\item 
\mbox{\hyperlink{class_qformer_1_1_bert_l_m_head_model_a3ef6c46ba303c1e5497fceeb0f8c2a81}{forward}} (self, input\+\_\+ids=None, attention\+\_\+mask=None, position\+\_\+ids=None, head\+\_\+mask=None, query\+\_\+embeds=None, encoder\+\_\+hidden\+\_\+states=None, encoder\+\_\+attention\+\_\+mask=None, labels=None, past\+\_\+key\+\_\+values=None, use\+\_\+cache=True, output\+\_\+attentions=None, output\+\_\+hidden\+\_\+states=None, return\+\_\+dict=None, return\+\_\+logits=False, is\+\_\+decoder=True, reduction="{}mean"{})
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_a6903669a7d439541d7ed9905ce42844e}\label{class_qformer_1_1_bert_l_m_head_model_a6903669a7d439541d7ed9905ce42844e} 
{\bfseries prepare\+\_\+inputs\+\_\+for\+\_\+generation} (self, input\+\_\+ids, query\+\_\+embeds, past=None, attention\+\_\+mask=None, \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}model\+\_\+kwargs)
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_ab141eeccef77285f78778725f8fd3b4a}\label{class_qformer_1_1_bert_l_m_head_model_ab141eeccef77285f78778725f8fd3b4a} 
{\bfseries bert} = \mbox{\hyperlink{class_qformer_1_1_bert_model}{Bert\+Model}}(config, add\+\_\+pooling\+\_\+layer=False)
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_aa26abf44519d1458851f0d9bf3fcef27}\label{class_qformer_1_1_bert_l_m_head_model_aa26abf44519d1458851f0d9bf3fcef27} 
{\bfseries cls} = \mbox{\hyperlink{class_qformer_1_1_bert_only_m_l_m_head}{Bert\+Only\+MLMHead}}(config)
\end{DoxyCompactItemize}
\doxysubsubsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_ace5dd22bfaae85e6810fd31897001c5f}\label{class_qformer_1_1_bert_l_m_head_model_ace5dd22bfaae85e6810fd31897001c5f} 
{\bfseries \+\_\+reorder\+\_\+cache} (self, past, beam\+\_\+idx)
\end{DoxyCompactItemize}
\doxysubsection*{Protected Member Functions inherited from \mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model}{Qformer.\+Bert\+Pre\+Trained\+Model}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model_a70e79487d3ccebe83df524463c1ff0b9}{\+\_\+init\+\_\+weights}} (self, module)
\begin{DoxyCompactList}\small\item\em 모듈의 가중치를 초기화합니다. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsubsection*{Static Protected Attributes}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_a4b9c2fda4ac11bee2f043aa0cbca7071}\label{class_qformer_1_1_bert_l_m_head_model_a4b9c2fda4ac11bee2f043aa0cbca7071} 
list {\bfseries \+\_\+keys\+\_\+to\+\_\+ignore\+\_\+on\+\_\+load\+\_\+unexpected} = \mbox{[}r"{}pooler"{}\mbox{]}
\end{DoxyCompactItemize}
\doxysubsection*{Static Protected Attributes inherited from \mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model}{Qformer.\+Bert\+Pre\+Trained\+Model}}}
\begin{DoxyCompactItemize}
\item 
list {\bfseries \+\_\+keys\+\_\+to\+\_\+ignore\+\_\+on\+\_\+load\+\_\+missing} = \mbox{[}r"{}position\+\_\+ids"{}\mbox{]}
\end{DoxyCompactItemize}
\doxysubsubsection*{Additional Inherited Members}
\doxysubsection*{Static Public Attributes inherited from \mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model}{Qformer.\+Bert\+Pre\+Trained\+Model}}}
\begin{DoxyCompactItemize}
\item 
{\bfseries config\+\_\+class} = Bert\+Config
\item 
str {\bfseries base\+\_\+model\+\_\+prefix} = "{}bert"{}
\end{DoxyCompactItemize}


\doxysubsection{Member Function Documentation}
\Hypertarget{class_qformer_1_1_bert_l_m_head_model_a3ef6c46ba303c1e5497fceeb0f8c2a81}\index{Qformer.BertLMHeadModel@{Qformer.BertLMHeadModel}!forward@{forward}}
\index{forward@{forward}!Qformer.BertLMHeadModel@{Qformer.BertLMHeadModel}}
\doxysubsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily \label{class_qformer_1_1_bert_l_m_head_model_a3ef6c46ba303c1e5497fceeb0f8c2a81} 
Qformer.\+Bert\+LMHead\+Model.\+forward (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{input\+\_\+ids}{ = {\ttfamily None}, }\item[{}]{attention\+\_\+mask}{ = {\ttfamily None}, }\item[{}]{position\+\_\+ids}{ = {\ttfamily None}, }\item[{}]{head\+\_\+mask}{ = {\ttfamily None}, }\item[{}]{query\+\_\+embeds}{ = {\ttfamily None}, }\item[{}]{encoder\+\_\+hidden\+\_\+states}{ = {\ttfamily None}, }\item[{}]{encoder\+\_\+attention\+\_\+mask}{ = {\ttfamily None}, }\item[{}]{labels}{ = {\ttfamily None}, }\item[{}]{past\+\_\+key\+\_\+values}{ = {\ttfamily None}, }\item[{}]{use\+\_\+cache}{ = {\ttfamily True}, }\item[{}]{output\+\_\+attentions}{ = {\ttfamily None}, }\item[{}]{output\+\_\+hidden\+\_\+states}{ = {\ttfamily None}, }\item[{}]{return\+\_\+dict}{ = {\ttfamily None}, }\item[{}]{return\+\_\+logits}{ = {\ttfamily False}, }\item[{}]{is\+\_\+decoder}{ = {\ttfamily True}, }\item[{}]{reduction}{ = {\ttfamily "{}mean"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):
    Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
    the model is configured as a decoder.
encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
    Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
    the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:
    - 1 for tokens that are **not masked**,
    - 0 for tokens that are **masked**.
labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
    Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
    ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are
    ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``
past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
    Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
    If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`
    (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`
    instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.
use_cache (:obj:`bool`, `optional`):
    If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up
    decoding (see :obj:`past_key_values`).
Returns:
Example::
    >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig
    >>> import torch
    >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
    >>> config = BertConfig.from_pretrained("bert-base-cased")
    >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)
    >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
    >>> outputs = model(**inputs)
    >>> prediction_logits = outputs.logits
\end{DoxyVerb}
 

The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
Qformer.\+py\end{DoxyCompactItemize}
