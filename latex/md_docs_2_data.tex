\chapter{Data}
\hypertarget{md_docs_2_data}{}\label{md_docs_2_data}\index{Data@{Data}}
\label{md_docs_2_data_autotoc_md3}%
\Hypertarget{md_docs_2_data_autotoc_md3}%


\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{2}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Data file name   }&\PBS\raggedleft \cellcolor{\tableheadbgcolor}\textbf{ Size    }\\\cline{1-2}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Data file name   }&\PBS\raggedleft \cellcolor{\tableheadbgcolor}\textbf{ Size    }\\\cline{1-2}
\endhead
\href{https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json}{\texttt{ llava\+\_\+instruct\+\_\+150k.\+json}}   &\PBS\raggedleft 229 MB    \\\cline{1-2}
\href{https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_80k.json}{\texttt{ llava\+\_\+instruct\+\_\+80k.\+json}}   &\PBS\raggedleft 229 MB    \\\cline{1-2}
\href{https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/conversation_58k.json}{\texttt{ conversation\+\_\+58k.\+json}}   &\PBS\raggedleft 126 MB    \\\cline{1-2}
\href{https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/detail_23k.json}{\texttt{ detail\+\_\+23k.\+json}}   &\PBS\raggedleft 20.\+5 MB    \\\cline{1-2}
\href{https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/complex_reasoning_77k.json}{\texttt{ complex\+\_\+reasoning\+\_\+77k.\+json}}   &\PBS\raggedleft 79.\+6 MB   \\\cline{1-2}
\end{longtabu}
\hypertarget{md_docs_2_data_autotoc_md4}{}\doxysubsection{\texorpdfstring{Pretraining Dataset}{Pretraining Dataset}}\label{md_docs_2_data_autotoc_md4}
The pretraining dataset used in this release is a subset of CC-\/3M dataset, filtered with a more balanced concept coverage distribution. Please see \href{https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K}{\texttt{ here}} for a detailed description of the dataset structure and how to download the images.

If you already have CC-\/3M dataset on your disk, the image names follow this format\+: {\ttfamily GCC\+\_\+train\+\_\+000000000.\+jpg}. You may edit the {\ttfamily image} field correspondingly if necessary.

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{4}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Data   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Chat File   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Meta Data   }&\PBS\raggedleft \cellcolor{\tableheadbgcolor}\textbf{ Size    }\\\cline{1-4}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Data   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Chat File   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Meta Data   }&\PBS\raggedleft \cellcolor{\tableheadbgcolor}\textbf{ Size    }\\\cline{1-4}
\endhead
CC-\/3M Concept-\/balanced 595K   &\href{https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/chat.json}{\texttt{ chat.\+json}}   &\href{https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/metadata.json}{\texttt{ metadata.\+json}}   &\PBS\raggedleft 211 MB    \\\cline{1-4}
LAION/\+CC/\+SBU BLIP-\/\+Caption Concept-\/balanced 558K   &\href{https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/blip_laion_cc_sbu_558k.json}{\texttt{ blip\+\_\+laion\+\_\+cc\+\_\+sbu\+\_\+558k.\+json}}   &"{}"{} "{}metadata.\+json"{}   &\PBS\raggedleft 181 MB   \\\cline{1-4}
\end{longtabu}


{\bfseries{Important notice}}\+: Upon the request from the community, as \texorpdfstring{$\sim$}{\string~}15\% images of the original CC-\/3M dataset are no longer accessible, we upload \href{https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/images.zip}{\texttt{ {\ttfamily images.\+zip}}} for better reproducing our work in research community. It must not be used for any other purposes. The use of these images must comply with the CC-\/3M license. This may be taken down at any time when requested by the original CC-\/3M dataset owner or owners of the referenced images.\hypertarget{md_docs_2_data_autotoc_md5}{}\doxysubsection{\texorpdfstring{GPT-\/4 Prompts}{GPT-\/4 Prompts}}\label{md_docs_2_data_autotoc_md5}
We provide our prompts and few-\/shot samples for GPT-\/4 queries, to better facilitate research in this domain. Please check out the \href{https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts}{\texttt{ {\ttfamily prompts}}} folder for three kinds of questions\+: conversation, detail description, and complex reasoning.

They are organized in a format of {\ttfamily system\+\_\+message.\+txt} for system message, pairs of {\ttfamily abc\+\_\+caps.\+txt} for few-\/shot sample user input, and {\ttfamily abc\+\_\+conv.\+txt} for few-\/shot sample reference output.

Note that you may find them in different format. For example, {\ttfamily conversation} is in {\ttfamily jsonl}, and detail description is answer-\/only. The selected format in our preliminary experiments works slightly better than a limited set of alternatives that we tried\+: {\ttfamily jsonl}, more natural format, answer-\/only. If interested, you may try other variants or conduct more careful study in this. Contributions are welcomed! 