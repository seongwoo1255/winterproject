\doxysection{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch Namespace Reference}
\hypertarget{namespacellama__flash__attn__monkey__patch}{}\label{namespacellama__flash__attn__monkey__patch}\index{llama\_flash\_attn\_monkey\_patch@{llama\_flash\_attn\_monkey\_patch}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
Tuple\mbox{[}torch.\+Tensor, Optional\mbox{[}torch.\+Tensor\mbox{]}, Optional\mbox{[}Tuple\mbox{[}torch.\+Tensor\mbox{]}\mbox{]}\mbox{]} \mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a5c68ef61a0287d74e55e329f382ad090}{forward}} (self, torch.\+Tensor hidden\+\_\+states, Optional\mbox{[}torch.\+Tensor\mbox{]} attention\+\_\+mask=None, Optional\mbox{[}torch.\+Tensor\mbox{]} position\+\_\+ids=None, Optional\mbox{[}Tuple\mbox{[}torch.\+Tensor\mbox{]}\mbox{]} past\+\_\+key\+\_\+value=None, bool output\+\_\+attentions=False, bool use\+\_\+cache=False)
\item 
\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7070583aef8b7fa93fe85f6792b5ee9d}{\+\_\+prepare\+\_\+decoder\+\_\+attention\+\_\+mask}} (self, attention\+\_\+mask, input\+\_\+shape, inputs\+\_\+embeds, past\+\_\+key\+\_\+values\+\_\+length)
\item 
\mbox{\hyperlink{namespacellama__flash__attn__monkey__patch_a7593dec7e8a06447dd33e4514015a0a4}{replace\+\_\+llama\+\_\+attn\+\_\+with\+\_\+flash\+\_\+attn}} ()
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacellama__flash__attn__monkey__patch_a7070583aef8b7fa93fe85f6792b5ee9d}\index{llama\_flash\_attn\_monkey\_patch@{llama\_flash\_attn\_monkey\_patch}!\_prepare\_decoder\_attention\_mask@{\_prepare\_decoder\_attention\_mask}}
\index{\_prepare\_decoder\_attention\_mask@{\_prepare\_decoder\_attention\_mask}!llama\_flash\_attn\_monkey\_patch@{llama\_flash\_attn\_monkey\_patch}}
\doxysubsubsection{\texorpdfstring{\_prepare\_decoder\_attention\_mask()}{\_prepare\_decoder\_attention\_mask()}}
{\footnotesize\ttfamily \label{namespacellama__flash__attn__monkey__patch_a7070583aef8b7fa93fe85f6792b5ee9d} 
llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+\_\+prepare\+\_\+decoder\+\_\+attention\+\_\+mask (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{attention\+\_\+mask}{, }\item[{}]{input\+\_\+shape}{, }\item[{}]{inputs\+\_\+embeds}{, }\item[{}]{past\+\_\+key\+\_\+values\+\_\+length}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{llama__flash__attn__monkey__patch_8py_source_l00098}{98}} of file \mbox{\hyperlink{llama__flash__attn__monkey__patch_8py_source}{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00100\ ):}
\DoxyCodeLine{00101\ \ \ \ \ \textcolor{comment}{\#\ [bsz,\ seq\_len]}}
\DoxyCodeLine{00102\ \ \ \ \ \textcolor{keywordflow}{return}\ attention\_mask}
\DoxyCodeLine{00103\ }
\DoxyCodeLine{00104\ }

\end{DoxyCode}
\Hypertarget{namespacellama__flash__attn__monkey__patch_a5c68ef61a0287d74e55e329f382ad090}\index{llama\_flash\_attn\_monkey\_patch@{llama\_flash\_attn\_monkey\_patch}!forward@{forward}}
\index{forward@{forward}!llama\_flash\_attn\_monkey\_patch@{llama\_flash\_attn\_monkey\_patch}}
\doxysubsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily \label{namespacellama__flash__attn__monkey__patch_a5c68ef61a0287d74e55e329f382ad090} 
 Tuple\mbox{[}torch.\+Tensor, Optional\mbox{[}torch.\+Tensor\mbox{]}, Optional\mbox{[}Tuple\mbox{[}torch.\+Tensor\mbox{]}\mbox{]}\mbox{]} llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+forward (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{torch.\+Tensor}]{hidden\+\_\+states}{, }\item[{Optional\mbox{[}torch.\+Tensor\mbox{]} }]{attention\+\_\+mask}{ = {\ttfamily None}, }\item[{Optional\mbox{[}torch.\+Tensor\mbox{]} }]{position\+\_\+ids}{ = {\ttfamily None}, }\item[{Optional\mbox{[}Tuple\mbox{[}torch.\+Tensor\mbox{]}\mbox{]} }]{past\+\_\+key\+\_\+value}{ = {\ttfamily None}, }\item[{bool }]{output\+\_\+attentions}{ = {\ttfamily False}, }\item[{bool }]{use\+\_\+cache}{ = {\ttfamily False}}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{llama__flash__attn__monkey__patch_8py_source_l00016}{16}} of file \mbox{\hyperlink{llama__flash__attn__monkey__patch_8py_source}{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00024\ )\ -\/>\ Tuple[torch.Tensor,\ Optional[torch.Tensor],\ Optional[Tuple[torch.Tensor]]]:}
\DoxyCodeLine{00025\ \ \ \ \ \textcolor{keywordflow}{if}\ output\_attentions:}
\DoxyCodeLine{00026\ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{00027\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Output\ attentions\ is\ not\ supported\ for\ patched\ \`{}LlamaAttention`,\ returning\ \`{}None`\ instead."{}}}
\DoxyCodeLine{00028\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{00029\ }
\DoxyCodeLine{00030\ \ \ \ \ bsz,\ q\_len,\ \_\ =\ hidden\_states.size()}
\DoxyCodeLine{00031\ }
\DoxyCodeLine{00032\ \ \ \ \ query\_states\ =\ (}
\DoxyCodeLine{00033\ \ \ \ \ \ \ \ \ self.q\_proj(hidden\_states)}
\DoxyCodeLine{00034\ \ \ \ \ \ \ \ \ .view(bsz,\ q\_len,\ self.num\_heads,\ self.head\_dim)}
\DoxyCodeLine{00035\ \ \ \ \ \ \ \ \ .transpose(1,\ 2)}
\DoxyCodeLine{00036\ \ \ \ \ )}
\DoxyCodeLine{00037\ \ \ \ \ key\_states\ =\ (}
\DoxyCodeLine{00038\ \ \ \ \ \ \ \ \ self.k\_proj(hidden\_states)}
\DoxyCodeLine{00039\ \ \ \ \ \ \ \ \ .view(bsz,\ q\_len,\ self.num\_key\_value\_heads,\ self.head\_dim)}
\DoxyCodeLine{00040\ \ \ \ \ \ \ \ \ .transpose(1,\ 2)}
\DoxyCodeLine{00041\ \ \ \ \ )}
\DoxyCodeLine{00042\ \ \ \ \ value\_states\ =\ (}
\DoxyCodeLine{00043\ \ \ \ \ \ \ \ \ self.v\_proj(hidden\_states)}
\DoxyCodeLine{00044\ \ \ \ \ \ \ \ \ .view(bsz,\ q\_len,\ self.num\_key\_value\_heads,\ self.head\_dim)}
\DoxyCodeLine{00045\ \ \ \ \ \ \ \ \ .transpose(1,\ 2)}
\DoxyCodeLine{00046\ \ \ \ \ )\ \ \textcolor{comment}{\#\ shape:\ (b,\ num\_heads,\ s,\ head\_dim)}}
\DoxyCodeLine{00047\ }
\DoxyCodeLine{00048\ \ \ \ \ kv\_seq\_len\ =\ key\_states.shape[-\/2]}
\DoxyCodeLine{00049\ \ \ \ \ \textcolor{keywordflow}{if}\ past\_key\_value\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{00050\ \ \ \ \ \ \ \ \ kv\_seq\_len\ +=\ past\_key\_value[0].shape[-\/2]}
\DoxyCodeLine{00051\ }
\DoxyCodeLine{00052\ \ \ \ \ cos,\ sin\ =\ self.rotary\_emb(value\_states,\ seq\_len=kv\_seq\_len)}
\DoxyCodeLine{00053\ \ \ \ \ query\_states,\ key\_states\ =\ apply\_rotary\_pos\_emb(}
\DoxyCodeLine{00054\ \ \ \ \ \ \ \ \ query\_states,\ key\_states,\ cos,\ sin,\ position\_ids}
\DoxyCodeLine{00055\ \ \ \ \ )}
\DoxyCodeLine{00056\ }
\DoxyCodeLine{00057\ \ \ \ \ \textcolor{keywordflow}{if}\ past\_key\_value\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{00058\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ reuse\ k,\ v}}
\DoxyCodeLine{00059\ \ \ \ \ \ \ \ \ key\_states\ =\ torch.cat([past\_key\_value[0],\ key\_states],\ dim=2)}
\DoxyCodeLine{00060\ \ \ \ \ \ \ \ \ value\_states\ =\ torch.cat([past\_key\_value[1],\ value\_states],\ dim=2)}
\DoxyCodeLine{00061\ }
\DoxyCodeLine{00062\ \ \ \ \ past\_key\_value\ =\ (key\_states,\ value\_states)\ \textcolor{keywordflow}{if}\ use\_cache\ \textcolor{keywordflow}{else}\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{00063\ }
\DoxyCodeLine{00064\ \ \ \ \ \textcolor{comment}{\#\ repeat\ k/v\ heads\ if\ n\_kv\_heads\ <\ n\_heads}}
\DoxyCodeLine{00065\ \ \ \ \ key\_states\ =\ repeat\_kv(key\_states,\ self.num\_key\_value\_groups)}
\DoxyCodeLine{00066\ \ \ \ \ value\_states\ =\ repeat\_kv(value\_states,\ self.num\_key\_value\_groups)}
\DoxyCodeLine{00067\ }
\DoxyCodeLine{00068\ \ \ \ \ \textcolor{comment}{\#\ Transform\ the\ data\ into\ the\ format\ required\ by\ flash\ attention}}
\DoxyCodeLine{00069\ \ \ \ \ qkv\ =\ torch.stack([query\_states,\ key\_states,\ value\_states],\ dim=2)}
\DoxyCodeLine{00070\ \ \ \ \ qkv\ =\ qkv.transpose(1,\ 3)\ \ \textcolor{comment}{\#\ shape:\ [b,\ s,\ 3,\ num\_heads,\ head\_dim]}}
\DoxyCodeLine{00071\ \ \ \ \ key\_padding\_mask\ =\ attention\_mask}
\DoxyCodeLine{00072\ }
\DoxyCodeLine{00073\ \ \ \ \ \textcolor{keywordflow}{if}\ key\_padding\_mask\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{00074\ \ \ \ \ \ \ \ \ qkv\ =\ qkv.reshape(-\/1,\ 3,\ self.num\_heads,\ self.head\_dim)}
\DoxyCodeLine{00075\ \ \ \ \ \ \ \ \ cu\_q\_lens\ =\ torch.arange(}
\DoxyCodeLine{00076\ \ \ \ \ \ \ \ \ \ \ \ \ 0,\ (bsz\ +\ 1)\ *\ q\_len,\ step=q\_len,\ dtype=torch.int32,\ device=qkv.device}
\DoxyCodeLine{00077\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{00078\ \ \ \ \ \ \ \ \ max\_s\ =\ q\_len}
\DoxyCodeLine{00079\ \ \ \ \ \ \ \ \ output\ =\ flash\_attn\_unpadded\_qkvpacked\_func(}
\DoxyCodeLine{00080\ \ \ \ \ \ \ \ \ \ \ \ \ qkv,\ cu\_q\_lens,\ max\_s,\ 0.0,\ softmax\_scale=\textcolor{keywordtype}{None},\ causal=\textcolor{keyword}{True}}
\DoxyCodeLine{00081\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{00082\ \ \ \ \ \ \ \ \ output\ =\ output.view(bsz,\ q\_len,\ -\/1)}
\DoxyCodeLine{00083\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{00084\ \ \ \ \ \ \ \ \ qkv\ =\ qkv.reshape(bsz,\ q\_len,\ -\/1)}
\DoxyCodeLine{00085\ \ \ \ \ \ \ \ \ qkv,\ indices,\ cu\_q\_lens,\ max\_s\ =\ unpad\_input(qkv,\ key\_padding\_mask)}
\DoxyCodeLine{00086\ \ \ \ \ \ \ \ \ qkv\ =\ qkv.view(-\/1,\ 3,\ self.num\_heads,\ self.head\_dim)}
\DoxyCodeLine{00087\ \ \ \ \ \ \ \ \ output\_unpad\ =\ flash\_attn\_unpadded\_qkvpacked\_func(}
\DoxyCodeLine{00088\ \ \ \ \ \ \ \ \ \ \ \ \ qkv,\ cu\_q\_lens,\ max\_s,\ 0.0,\ softmax\_scale=\textcolor{keywordtype}{None},\ causal=\textcolor{keyword}{True}}
\DoxyCodeLine{00089\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{00090\ \ \ \ \ \ \ \ \ output\_unpad\ =\ output\_unpad.reshape(-\/1,\ self.num\_heads\ *\ self.head\_dim)}
\DoxyCodeLine{00091\ \ \ \ \ \ \ \ \ output\ =\ pad\_input(output\_unpad,\ indices,\ bsz,\ q\_len)}
\DoxyCodeLine{00092\ }
\DoxyCodeLine{00093\ \ \ \ \ \textcolor{keywordflow}{return}\ self.o\_proj(output),\ \textcolor{keywordtype}{None},\ past\_key\_value}
\DoxyCodeLine{00094\ }
\DoxyCodeLine{00095\ }
\DoxyCodeLine{00096\ \textcolor{comment}{\#\ Disable\ the\ transformation\ of\ the\ attention\ mask\ in\ LlamaModel\ as\ the\ flash\ attention}}
\DoxyCodeLine{00097\ \textcolor{comment}{\#\ requires\ the\ attention\ mask\ to\ be\ the\ same\ as\ the\ key\_padding\_mask}}

\end{DoxyCode}
\Hypertarget{namespacellama__flash__attn__monkey__patch_a7593dec7e8a06447dd33e4514015a0a4}\index{llama\_flash\_attn\_monkey\_patch@{llama\_flash\_attn\_monkey\_patch}!replace\_llama\_attn\_with\_flash\_attn@{replace\_llama\_attn\_with\_flash\_attn}}
\index{replace\_llama\_attn\_with\_flash\_attn@{replace\_llama\_attn\_with\_flash\_attn}!llama\_flash\_attn\_monkey\_patch@{llama\_flash\_attn\_monkey\_patch}}
\doxysubsubsection{\texorpdfstring{replace\_llama\_attn\_with\_flash\_attn()}{replace\_llama\_attn\_with\_flash\_attn()}}
{\footnotesize\ttfamily \label{namespacellama__flash__attn__monkey__patch_a7593dec7e8a06447dd33e4514015a0a4} 
llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+replace\+\_\+llama\+\_\+attn\+\_\+with\+\_\+flash\+\_\+attn (\begin{DoxyParamCaption}{}{}\end{DoxyParamCaption})}



Definition at line \mbox{\hyperlink{llama__flash__attn__monkey__patch_8py_source_l00105}{105}} of file \mbox{\hyperlink{llama__flash__attn__monkey__patch_8py_source}{llama\+\_\+flash\+\_\+attn\+\_\+monkey\+\_\+patch.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00105\ \textcolor{keyword}{def\ }replace\_llama\_attn\_with\_flash\_attn():}
\DoxyCodeLine{00106\ \ \ \ \ cuda\_major,\ cuda\_minor\ =\ torch.cuda.get\_device\_capability()}
\DoxyCodeLine{00107\ \ \ \ \ \textcolor{keywordflow}{if}\ cuda\_major\ <\ 8:}
\DoxyCodeLine{00108\ \ \ \ \ \ \ \ \ warnings.warn(}
\DoxyCodeLine{00109\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Flash\ attention\ is\ only\ supported\ on\ A100\ or\ H100\ GPU\ during\ training\ due\ to\ head\ dim\ >\ 64\ backward."{}}}
\DoxyCodeLine{00110\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}ref:\ https://github.com/HazyResearch/flash-\/attention/issues/190\#issuecomment-\/1523359593"{}}}
\DoxyCodeLine{00111\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{00112\ \ \ \ \ transformers.models.llama.modeling\_llama.LlamaModel.\_prepare\_decoder\_attention\_mask\ =\ (}
\DoxyCodeLine{00113\ \ \ \ \ \ \ \ \ \_prepare\_decoder\_attention\_mask}
\DoxyCodeLine{00114\ \ \ \ \ )}
\DoxyCodeLine{00115\ \ \ \ \ transformers.models.llama.modeling\_llama.LlamaAttention.forward\ =\ forward}

\end{DoxyCode}
