\doxysection{Qformer.\+Bert\+Model Class Reference}
\hypertarget{class_qformer_1_1_bert_model}{}\label{class_qformer_1_1_bert_model}\index{Qformer.BertModel@{Qformer.BertModel}}
Inheritance diagram for Qformer.\+Bert\+Model\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=3.000000cm]{class_qformer_1_1_bert_model}
\end{center}
\end{figure}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{class_qformer_1_1_bert_model_abcec579fb60136bdf89518e1d5615ef2}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, config, add\+\_\+pooling\+\_\+layer=False)
\begin{DoxyCompactList}\small\item\em BERT 모델 초기화. \end{DoxyCompactList}\item 
\mbox{\hyperlink{class_qformer_1_1_bert_model_a69e843decdab9c86cfa6c366c41431e6}{get\+\_\+input\+\_\+embeddings}} (self)
\begin{DoxyCompactList}\small\item\em 입력 임베딩을 반환합니다. \end{DoxyCompactList}\item 
\mbox{\hyperlink{class_qformer_1_1_bert_model_ad229f1a904be19e626a663515e56a525}{set\+\_\+input\+\_\+embeddings}} (self, value)
\begin{DoxyCompactList}\small\item\em 입력 임베딩을 설정합니다. \end{DoxyCompactList}\item 
Tensor \mbox{\hyperlink{class_qformer_1_1_bert_model_a900b426e3390646031fe46b6e2db25d5}{get\+\_\+extended\+\_\+attention\+\_\+mask}} (self, Tensor attention\+\_\+mask, Tuple\mbox{[}int\mbox{]} input\+\_\+shape, device device, bool is\+\_\+decoder, bool has\+\_\+query=False)
\begin{DoxyCompactList}\small\item\em 확장된 어텐션 마스크를 생성합니다. \end{DoxyCompactList}\item 
\mbox{\hyperlink{class_qformer_1_1_bert_model_a0bc2b2aa249885dc0c8f9e91526b8361}{forward}} (self, input\+\_\+ids=None, attention\+\_\+mask=None, position\+\_\+ids=None, head\+\_\+mask=None, query\+\_\+embeds=None, encoder\+\_\+hidden\+\_\+states=None, encoder\+\_\+attention\+\_\+mask=None, past\+\_\+key\+\_\+values=None, use\+\_\+cache=None, output\+\_\+attentions=None, output\+\_\+hidden\+\_\+states=None, return\+\_\+dict=None, is\+\_\+decoder=False)
\begin{DoxyCompactList}\small\item\em 모델의 순전파 함수. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{class_qformer_1_1_bert_model_a7f19967aa1177a9561b5be5b938fc268}\label{class_qformer_1_1_bert_model_a7f19967aa1177a9561b5be5b938fc268} 
{\bfseries config} = config
\item 
\Hypertarget{class_qformer_1_1_bert_model_aeefad27e30986baebdf3d60c287f3a14}\label{class_qformer_1_1_bert_model_aeefad27e30986baebdf3d60c287f3a14} 
{\bfseries embeddings} = \mbox{\hyperlink{class_qformer_1_1_bert_embeddings}{Bert\+Embeddings}}(config)
\item 
\Hypertarget{class_qformer_1_1_bert_model_a0aadbcd4095680df41b2dc638e4c2136}\label{class_qformer_1_1_bert_model_a0aadbcd4095680df41b2dc638e4c2136} 
{\bfseries encoder} = \mbox{\hyperlink{class_qformer_1_1_bert_encoder}{Bert\+Encoder}}(config)
\item 
\Hypertarget{class_qformer_1_1_bert_model_ab9f6b7c11e0202002397d99d7dcdac69}\label{class_qformer_1_1_bert_model_ab9f6b7c11e0202002397d99d7dcdac69} 
{\bfseries pooler} = \mbox{\hyperlink{class_qformer_1_1_bert_pooler}{Bert\+Pooler}}(config) if add\+\_\+pooling\+\_\+layer else None
\end{DoxyCompactItemize}
\doxysubsubsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{class_qformer_1_1_bert_model_abd4539b6e5fb86d4a8eb0a20082c57e2}{\+\_\+prune\+\_\+heads}} (self, heads\+\_\+to\+\_\+prune)
\begin{DoxyCompactList}\small\item\em 특정 레이어의 어텐션 헤드를 가지치기합니다. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Protected Member Functions inherited from \mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model}{Qformer.\+Bert\+Pre\+Trained\+Model}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model_a70e79487d3ccebe83df524463c1ff0b9}{\+\_\+init\+\_\+weights}} (self, module)
\begin{DoxyCompactList}\small\item\em 모듈의 가중치를 초기화합니다. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsubsection*{Additional Inherited Members}
\doxysubsection*{Static Public Attributes inherited from \mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model}{Qformer.\+Bert\+Pre\+Trained\+Model}}}
\begin{DoxyCompactItemize}
\item 
{\bfseries config\+\_\+class} = Bert\+Config
\item 
str {\bfseries base\+\_\+model\+\_\+prefix} = "{}bert"{}
\end{DoxyCompactItemize}
\doxysubsection*{Static Protected Attributes inherited from \mbox{\hyperlink{class_qformer_1_1_bert_pre_trained_model}{Qformer.\+Bert\+Pre\+Trained\+Model}}}
\begin{DoxyCompactItemize}
\item 
list {\bfseries \+\_\+keys\+\_\+to\+\_\+ignore\+\_\+on\+\_\+load\+\_\+missing} = \mbox{[}r"{}position\+\_\+ids"{}\mbox{]}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
cross-attention is added between the self-attention layers, following the architecture described in `Attention is
all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an
input to the forward pass.
\end{DoxyVerb}
 

\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{class_qformer_1_1_bert_model_abcec579fb60136bdf89518e1d5615ef2}\index{Qformer.BertModel@{Qformer.BertModel}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!Qformer.BertModel@{Qformer.BertModel}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{class_qformer_1_1_bert_model_abcec579fb60136bdf89518e1d5615ef2} 
Qformer.\+Bert\+Model.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{config}{, }\item[{}]{add\+\_\+pooling\+\_\+layer}{ = {\ttfamily False}}\end{DoxyParamCaption})}



BERT 모델 초기화. 

임베딩, 인코더, 풀링 레이어를 초기화하며, 필요 시 가중치를 초기화합니다.


\begin{DoxyParams}{Parameters}
{\em config} & BERT 구성 객체. \\
\hline
{\em add\+\_\+pooling\+\_\+layer} & 풀링 레이어 추가 여부. \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\Hypertarget{class_qformer_1_1_bert_model_abd4539b6e5fb86d4a8eb0a20082c57e2}\index{Qformer.BertModel@{Qformer.BertModel}!\_prune\_heads@{\_prune\_heads}}
\index{\_prune\_heads@{\_prune\_heads}!Qformer.BertModel@{Qformer.BertModel}}
\doxysubsubsection{\texorpdfstring{\_prune\_heads()}{\_prune\_heads()}}
{\footnotesize\ttfamily \label{class_qformer_1_1_bert_model_abd4539b6e5fb86d4a8eb0a20082c57e2} 
Qformer.\+Bert\+Model.\+\_\+prune\+\_\+heads (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{heads\+\_\+to\+\_\+prune}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}



특정 레이어의 어텐션 헤드를 가지치기합니다. 

지정된 레이어의 어텐션 헤드 수를 줄입니다.


\begin{DoxyParams}{Parameters}
{\em heads\+\_\+to\+\_\+prune} & 가지치기할 헤드의 딕셔너리 (\{layer\+\_\+num\+: list of heads\}).\\
\hline
\end{DoxyParams}
\begin{DoxyVerb}Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base
class PreTrainedModel
\end{DoxyVerb}
 \Hypertarget{class_qformer_1_1_bert_model_a0bc2b2aa249885dc0c8f9e91526b8361}\index{Qformer.BertModel@{Qformer.BertModel}!forward@{forward}}
\index{forward@{forward}!Qformer.BertModel@{Qformer.BertModel}}
\doxysubsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily \label{class_qformer_1_1_bert_model_a0bc2b2aa249885dc0c8f9e91526b8361} 
Qformer.\+Bert\+Model.\+forward (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{input\+\_\+ids}{ = {\ttfamily None}, }\item[{}]{attention\+\_\+mask}{ = {\ttfamily None}, }\item[{}]{position\+\_\+ids}{ = {\ttfamily None}, }\item[{}]{head\+\_\+mask}{ = {\ttfamily None}, }\item[{}]{query\+\_\+embeds}{ = {\ttfamily None}, }\item[{}]{encoder\+\_\+hidden\+\_\+states}{ = {\ttfamily None}, }\item[{}]{encoder\+\_\+attention\+\_\+mask}{ = {\ttfamily None}, }\item[{}]{past\+\_\+key\+\_\+values}{ = {\ttfamily None}, }\item[{}]{use\+\_\+cache}{ = {\ttfamily None}, }\item[{}]{output\+\_\+attentions}{ = {\ttfamily None}, }\item[{}]{output\+\_\+hidden\+\_\+states}{ = {\ttfamily None}, }\item[{}]{return\+\_\+dict}{ = {\ttfamily None}, }\item[{}]{is\+\_\+decoder}{ = {\ttfamily False}}\end{DoxyParamCaption})}



모델의 순전파 함수. 

입력 데이터를 처리하여 최종 히든 상태, 풀링 출력, 어텐션 맵 등을 반환합니다.


\begin{DoxyParams}{Parameters}
{\em input\+\_\+ids} & 입력 ID (배치 크기 x 시퀀스 길이). \\
\hline
{\em attention\+\_\+mask} & 어텐션 마스크. \\
\hline
{\em position\+\_\+ids} & 위치 ID (선택적). \\
\hline
{\em head\+\_\+mask} & 어텐션 헤드 마스크 (선택적). \\
\hline
{\em query\+\_\+embeds} & 질의 임베딩 (선택적). \\
\hline
{\em encoder\+\_\+hidden\+\_\+states} & 인코더 히든 상태 (선택적, 디코더일 경우 필요). \\
\hline
{\em encoder\+\_\+attention\+\_\+mask} & 인코더 어텐션 마스크 (선택적). \\
\hline
{\em past\+\_\+key\+\_\+values} & 이전 Key/\+Value 정보 (선택적, 디코더일 경우 사용). \\
\hline
{\em use\+\_\+cache} & 캐싱 사용 여부 (디코더의 경우 속도 최적화를 위해 사용). \\
\hline
{\em output\+\_\+attentions} & 어텐션 맵 반환 여부. \\
\hline
{\em output\+\_\+hidden\+\_\+states} & 각 레이어의 히든 상태 반환 여부. \\
\hline
{\em return\+\_\+dict} & 결과를 딕셔너리 형태로 반환 여부. \\
\hline
{\em is\+\_\+decoder} & 디코더 여부. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
최종 출력 및 선택적 정보 (히든 상태, 풀링 출력, 어텐션 맵 등).
\end{DoxyReturn}
\begin{DoxyVerb}    encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):
        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
        the model is configured as a decoder.
    encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
        the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:
        - 1 for tokens that are **not masked**,
        - 0 for tokens that are **masked**.
    past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
        If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`
        (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`
        instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.
    use_cache (:obj:`bool`, `optional`):
        If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up
        decoding (see :obj:`past_key_values`).
\end{DoxyVerb}
 \Hypertarget{class_qformer_1_1_bert_model_a900b426e3390646031fe46b6e2db25d5}\index{Qformer.BertModel@{Qformer.BertModel}!get\_extended\_attention\_mask@{get\_extended\_attention\_mask}}
\index{get\_extended\_attention\_mask@{get\_extended\_attention\_mask}!Qformer.BertModel@{Qformer.BertModel}}
\doxysubsubsection{\texorpdfstring{get\_extended\_attention\_mask()}{get\_extended\_attention\_mask()}}
{\footnotesize\ttfamily \label{class_qformer_1_1_bert_model_a900b426e3390646031fe46b6e2db25d5} 
 Tensor Qformer.\+Bert\+Model.\+get\+\_\+extended\+\_\+attention\+\_\+mask (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{Tensor}]{attention\+\_\+mask}{, }\item[{Tuple\mbox{[}int\mbox{]}}]{input\+\_\+shape}{, }\item[{device}]{device}{, }\item[{bool}]{is\+\_\+decoder}{, }\item[{bool }]{has\+\_\+query}{ = {\ttfamily False}}\end{DoxyParamCaption})}



확장된 어텐션 마스크를 생성합니다. 

입력 마스크를 확장하여 브로드캐스트 가능하게 만들고, 디코더의 경우 인과 마스크를 추가합니다.


\begin{DoxyParams}{Parameters}
{\em attention\+\_\+mask} & 입력 어텐션 마스크. \\
\hline
{\em input\+\_\+shape} & 입력 데이터의 형태. \\
\hline
{\em device} & 입력 데이터가 위치한 디바이스. \\
\hline
{\em is\+\_\+decoder} & 디코더 여부. \\
\hline
{\em has\+\_\+query} & 질의 여부 (Uni\+LM 스타일 지원). \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
확장된 어텐션 마스크.
\end{DoxyReturn}
\begin{DoxyVerb}Makes broadcastable attention and causal masks so that future and masked tokens are ignored.

Arguments:
    attention_mask (:obj:`torch.Tensor`):
        Mask with ones indicating tokens to attend to, zeros for tokens to ignore.
    input_shape (:obj:`Tuple[int]`):
        The shape of the input to the model.
    device: (:obj:`torch.device`):
        The device of the input to the model.

Returns:
    :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.
\end{DoxyVerb}
 \Hypertarget{class_qformer_1_1_bert_model_a69e843decdab9c86cfa6c366c41431e6}\index{Qformer.BertModel@{Qformer.BertModel}!get\_input\_embeddings@{get\_input\_embeddings}}
\index{get\_input\_embeddings@{get\_input\_embeddings}!Qformer.BertModel@{Qformer.BertModel}}
\doxysubsubsection{\texorpdfstring{get\_input\_embeddings()}{get\_input\_embeddings()}}
{\footnotesize\ttfamily \label{class_qformer_1_1_bert_model_a69e843decdab9c86cfa6c366c41431e6} 
Qformer.\+Bert\+Model.\+get\+\_\+input\+\_\+embeddings (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}



입력 임베딩을 반환합니다. 

\begin{DoxyReturn}{Returns}
입력 임베딩 레이어. 
\end{DoxyReturn}
\Hypertarget{class_qformer_1_1_bert_model_ad229f1a904be19e626a663515e56a525}\index{Qformer.BertModel@{Qformer.BertModel}!set\_input\_embeddings@{set\_input\_embeddings}}
\index{set\_input\_embeddings@{set\_input\_embeddings}!Qformer.BertModel@{Qformer.BertModel}}
\doxysubsubsection{\texorpdfstring{set\_input\_embeddings()}{set\_input\_embeddings()}}
{\footnotesize\ttfamily \label{class_qformer_1_1_bert_model_ad229f1a904be19e626a663515e56a525} 
Qformer.\+Bert\+Model.\+set\+\_\+input\+\_\+embeddings (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{value}{}\end{DoxyParamCaption})}



입력 임베딩을 설정합니다. 


\begin{DoxyParams}{Parameters}
{\em value} & 새로운 임베딩 레이어. \\
\hline
\end{DoxyParams}


The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
Qformer.\+py\end{DoxyCompactItemize}
