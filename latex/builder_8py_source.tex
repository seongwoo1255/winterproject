\doxysection{builder.\+py}
\hypertarget{builder_8py_source}{}\label{builder_8py_source}\mbox{\hyperlink{builder_8py}{Go to the documentation of this file.}}
\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00001}\mbox{\hyperlink{namespacellava_1_1model_1_1builder}{00001}}\ \textcolor{comment}{\#\ \ \ \ Copyright\ 2023\ Haotian\ Liu}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00002}00002\ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00003}00003\ \textcolor{comment}{\#\ \ \ \ Licensed\ under\ the\ Apache\ License,\ Version\ 2.0\ (the\ "{}License"{});}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00004}00004\ \textcolor{comment}{\#\ \ \ \ you\ may\ not\ use\ this\ file\ except\ in\ compliance\ with\ the\ License.}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00005}00005\ \textcolor{comment}{\#\ \ \ \ You\ may\ obtain\ a\ copy\ of\ the\ License\ at}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00006}00006\ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00007}00007\ \textcolor{comment}{\#\ \ \ \ \ \ \ \ http://www.apache.org/licenses/LICENSE-\/2.0}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00008}00008\ \textcolor{comment}{\#}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00009}00009\ \textcolor{comment}{\#\ \ \ \ Unless\ required\ by\ applicable\ law\ or\ agreed\ to\ in\ writing,\ software}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00010}00010\ \textcolor{comment}{\#\ \ \ \ distributed\ under\ the\ License\ is\ distributed\ on\ an\ "{}AS\ IS"{}\ BASIS,}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00011}00011\ \textcolor{comment}{\#\ \ \ \ WITHOUT\ WARRANTIES\ OR\ CONDITIONS\ OF\ ANY\ KIND,\ either\ express\ or\ implied.}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00012}00012\ \textcolor{comment}{\#\ \ \ \ See\ the\ License\ for\ the\ specific\ language\ governing\ permissions\ and}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00013}00013\ \textcolor{comment}{\#\ \ \ \ limitations\ under\ the\ License.}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00014}00014\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00015}00015\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00020}00020\ \textcolor{keyword}{import}\ os}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00021}00021\ \textcolor{keyword}{import}\ warnings}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00022}00022\ \textcolor{keyword}{import}\ shutil}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00023}00023\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00024}00024\ \textcolor{keyword}{from}\ transformers\ \textcolor{keyword}{import}\ AutoTokenizer,\ AutoModelForCausalLM,\ AutoConfig,\ BitsAndBytesConfig}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00025}00025\ \textcolor{keyword}{import}\ torch}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00026}00026\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacellava_1_1model}{llava.model}}\ \textcolor{keyword}{import}\ *}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00027}00027\ \textcolor{keyword}{from}\ llava.constants\ \textcolor{keyword}{import}\ DEFAULT\_IMAGE\_PATCH\_TOKEN,\ DEFAULT\_IM\_START\_TOKEN,\ DEFAULT\_IM\_END\_TOKEN}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00028}00028\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00029}00029\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00043}\mbox{\hyperlink{namespacellava_1_1model_1_1builder_a017ac83406bb503ad6d09d2a895f013b}{00043}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespacellava_1_1model_1_1builder_a017ac83406bb503ad6d09d2a895f013b}{load\_pretrained\_model}}(model\_path,\ model\_base,\ model\_name,\ load\_8bit=False,\ load\_4bit=False,\ device\_map="{}auto"{},\ device="{}cuda"{},\ use\_flash\_attn=False,\ **kwargs):}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00044}00044\ \ \ \ \ kwargs\ =\ \{\textcolor{stringliteral}{"{}device\_map"{}}:\ device\_map,\ **kwargs\}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00045}00045\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00046}00046\ \ \ \ \ \textcolor{keywordflow}{if}\ device\ !=\ \textcolor{stringliteral}{"{}cuda"{}}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00047}00047\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{'device\_map'}]\ =\ \{\textcolor{stringliteral}{"{}"{}}:\ device\}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00048}00048\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00049}00049\ \ \ \ \ \textcolor{keywordflow}{if}\ load\_8bit:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00050}00050\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{'load\_in\_8bit'}]\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00051}00051\ \ \ \ \ \textcolor{keywordflow}{elif}\ load\_4bit:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00052}00052\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{'load\_in\_4bit'}]\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00053}00053\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{'quantization\_config'}]\ =\ BitsAndBytesConfig(}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00054}00054\ \ \ \ \ \ \ \ \ \ \ \ \ load\_in\_4bit=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00055}00055\ \ \ \ \ \ \ \ \ \ \ \ \ bnb\_4bit\_compute\_dtype=torch.float16,}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00056}00056\ \ \ \ \ \ \ \ \ \ \ \ \ bnb\_4bit\_use\_double\_quant=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00057}00057\ \ \ \ \ \ \ \ \ \ \ \ \ bnb\_4bit\_quant\_type=\textcolor{stringliteral}{'nf4'}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00058}00058\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00059}00059\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00060}00060\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{'torch\_dtype'}]\ =\ torch.float16}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00061}00061\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00062}00062\ \ \ \ \ \textcolor{keywordflow}{if}\ use\_flash\_attn:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00063}00063\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{'attn\_implementation'}]\ =\ \textcolor{stringliteral}{'flash\_attention\_2'}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00064}00064\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00065}00065\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{'llava'}\ \textcolor{keywordflow}{in}\ model\_name.lower():}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00066}00066\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Load\ LLaVA\ model}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00067}00067\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{'lora'}\ \textcolor{keywordflow}{in}\ model\_name.lower()\ \textcolor{keywordflow}{and}\ model\_base\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00068}00068\ \ \ \ \ \ \ \ \ \ \ \ \ warnings.warn(\textcolor{stringliteral}{'There\ is\ \`{}lora`\ in\ model\ name\ but\ no\ \`{}model\_base`\ is\ provided.\ If\ you\ are\ loading\ a\ LoRA\ model,\ please\ provide\ the\ \`{}model\_base`\ argument.\ Detailed\ instruction:\ https://github.com/haotian-\/liu/LLaVA\#launch-\/a-\/model-\/worker-\/lora-\/weights-\/unmerged.'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00069}00069\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{'lora'}\ \textcolor{keywordflow}{in}\ model\_name.lower()\ \textcolor{keywordflow}{and}\ model\_base\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00070}00070\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{from}\ llava.model.language\_model.llava\_llama\ \textcolor{keyword}{import}\ LlavaConfig}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00071}00071\ \ \ \ \ \ \ \ \ \ \ \ \ lora\_cfg\_pretrained\ =\ LlavaConfig.from\_pretrained(model\_path)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00072}00072\ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_base,\ use\_fast=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00073}00073\ \ \ \ \ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'Loading\ LLaVA\ from\ base\ model...'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00074}00074\ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ LlavaLlamaForCausalLM.from\_pretrained(model\_base,\ low\_cpu\_mem\_usage=\textcolor{keyword}{True},\ config=lora\_cfg\_pretrained,\ **kwargs)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ \ \ \ \ token\_num,\ tokem\_dim\ =\ model.lm\_head.out\_features,\ model.lm\_head.in\_features}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00076}00076\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ model.lm\_head.weight.shape[0]\ !=\ token\_num:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00077}00077\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model.lm\_head.weight\ =\ torch.nn.Parameter(torch.empty(token\_num,\ tokem\_dim,\ device=model.device,\ dtype=model.dtype))}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00078}00078\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model.model.embed\_tokens.weight\ =\ torch.nn.Parameter(torch.empty(token\_num,\ tokem\_dim,\ device=model.device,\ dtype=model.dtype))}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00079}00079\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00080}00080\ \ \ \ \ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'Loading\ additional\ LLaVA\ weights...'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00081}00081\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ os.path.exists(os.path.join(model\_path,\ \textcolor{stringliteral}{'non\_lora\_trainables.bin'})):}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00082}00082\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ non\_lora\_trainables\ =\ torch.load(os.path.join(model\_path,\ \textcolor{stringliteral}{'non\_lora\_trainables.bin'}),\ map\_location=\textcolor{stringliteral}{'cpu'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00083}00083\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00084}00084\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ this\ is\ probably\ from\ HF\ Hub}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00085}00085\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{from}\ huggingface\_hub\ \textcolor{keyword}{import}\ hf\_hub\_download}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00086}00086\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{def\ }load\_from\_hf(repo\_id,\ filename,\ subfolder=None):}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00087}00087\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ cache\_file\ =\ hf\_hub\_download(}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00088}00088\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ repo\_id=repo\_id,}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00089}00089\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ filename=filename,}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00090}00090\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ subfolder=subfolder)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00091}00091\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ torch.load(cache\_file,\ map\_location=\textcolor{stringliteral}{'cpu'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00092}00092\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ non\_lora\_trainables\ =\ load\_from\_hf(model\_path,\ \textcolor{stringliteral}{'non\_lora\_trainables.bin'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00093}00093\ \ \ \ \ \ \ \ \ \ \ \ \ non\_lora\_trainables\ =\ \{(k[11:]\ \textcolor{keywordflow}{if}\ k.startswith(\textcolor{stringliteral}{'base\_model.'})\ \textcolor{keywordflow}{else}\ k):\ v\ \textcolor{keywordflow}{for}\ k,\ v\ \textcolor{keywordflow}{in}\ non\_lora\_trainables.items()\}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00094}00094\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ any(k.startswith(\textcolor{stringliteral}{'model.model.'})\ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ non\_lora\_trainables):}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00095}00095\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ non\_lora\_trainables\ =\ \{(k[6:]\ \textcolor{keywordflow}{if}\ k.startswith(\textcolor{stringliteral}{'model.'})\ \textcolor{keywordflow}{else}\ k):\ v\ \textcolor{keywordflow}{for}\ k,\ v\ \textcolor{keywordflow}{in}\ non\_lora\_trainables.items()\}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00096}00096\ \ \ \ \ \ \ \ \ \ \ \ \ model.load\_state\_dict(non\_lora\_trainables,\ strict=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00097}00097\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00098}00098\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{from}\ peft\ \textcolor{keyword}{import}\ PeftModel}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00099}00099\ \ \ \ \ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'Loading\ LoRA\ weights...'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00100}00100\ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ PeftModel.from\_pretrained(model,\ model\_path)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00101}00101\ \ \ \ \ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'Merging\ LoRA\ weights...'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00102}00102\ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ model.merge\_and\_unload()}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00103}00103\ \ \ \ \ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'Model\ is\ loaded...'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00104}00104\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ model\_base\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00105}00105\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ this\ may\ be\ mm\ projector\ only}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00106}00106\ \ \ \ \ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'Loading\ LLaVA\ from\ base\ model...'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00107}00107\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{'mpt'}\ \textcolor{keywordflow}{in}\ model\_name.lower():}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00108}00108\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ os.path.isfile(os.path.join(model\_path,\ \textcolor{stringliteral}{'configuration\_mpt.py'})):}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00109}00109\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ shutil.copyfile(os.path.join(model\_base,\ \textcolor{stringliteral}{'configuration\_mpt.py'}),\ os.path.join(model\_path,\ \textcolor{stringliteral}{'configuration\_mpt.py'}))}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00110}00110\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_base,\ use\_fast=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00111}00111\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ cfg\_pretrained\ =\ AutoConfig.from\_pretrained(model\_path,\ trust\_remote\_code=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00112}00112\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ LlavaMptForCausalLM.from\_pretrained(model\_base,\ low\_cpu\_mem\_usage=\textcolor{keyword}{True},\ config=cfg\_pretrained,\ **kwargs)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00113}00113\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00114}00114\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_base,\ use\_fast=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00115}00115\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ cfg\_pretrained\ =\ AutoConfig.from\_pretrained(model\_path)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00116}00116\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ LlavaLlamaForCausalLM.from\_pretrained(model\_base,\ low\_cpu\_mem\_usage=\textcolor{keyword}{True},\ config=cfg\_pretrained,\ **kwargs)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00117}00117\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00118}00118\ \ \ \ \ \ \ \ \ \ \ \ \ mm\_projector\_weights\ =\ torch.load(os.path.join(model\_path,\ \textcolor{stringliteral}{'mm\_projector.bin'}),\ map\_location=\textcolor{stringliteral}{'cpu'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00119}00119\ \ \ \ \ \ \ \ \ \ \ \ \ mm\_projector\_weights\ =\ \{k:\ v.to(torch.float16)\ \textcolor{keywordflow}{for}\ k,\ v\ \textcolor{keywordflow}{in}\ mm\_projector\_weights.items()\}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00120}00120\ \ \ \ \ \ \ \ \ \ \ \ \ model.load\_state\_dict(mm\_projector\_weights,\ strict=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00121}00121\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00122}00122\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{'mpt'}\ \textcolor{keywordflow}{in}\ model\_name.lower():}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_path,\ use\_fast=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00124}00124\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ LlavaMptForCausalLM.from\_pretrained(model\_path,\ low\_cpu\_mem\_usage=\textcolor{keyword}{True},\ **kwargs)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00125}00125\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ \textcolor{stringliteral}{'mistral'}\ \textcolor{keywordflow}{in}\ model\_name.lower():}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00126}00126\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_path)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00127}00127\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ LlavaMistralForCausalLM.from\_pretrained(}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00128}00128\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\_path,}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00129}00129\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ low\_cpu\_mem\_usage=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00130}00130\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **kwargs}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00132}00132\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00133}00133\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_path,\ use\_fast=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00134}00134\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ LlavaLlamaForCausalLM.from\_pretrained(}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00135}00135\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\_path,}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00136}00136\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ low\_cpu\_mem\_usage=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00137}00137\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **kwargs}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00138}00138\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00139}00139\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Load\ language\ model}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00141}00141\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ model\_base\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00142}00142\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ PEFT\ model}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00143}00143\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{from}\ peft\ \textcolor{keyword}{import}\ PeftModel}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00144}00144\ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_base,\ use\_fast=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00145}00145\ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ AutoModelForCausalLM.from\_pretrained(model\_base,\ low\_cpu\_mem\_usage=\textcolor{keyword}{True},\ **kwargs)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00146}00146\ \ \ \ \ \ \ \ \ \ \ \ \ print(f\textcolor{stringliteral}{"{}Loading\ LoRA\ weights\ from\ \{model\_path\}"{}})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00147}00147\ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ PeftModel.from\_pretrained(model,\ model\_path)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00148}00148\ \ \ \ \ \ \ \ \ \ \ \ \ print(f\textcolor{stringliteral}{"{}Merging\ weights"{}})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00149}00149\ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ model.merge\_and\_unload()}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00150}00150\ \ \ \ \ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'Convert\ to\ FP16...'})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00151}00151\ \ \ \ \ \ \ \ \ \ \ \ \ model.to(torch.float16)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00152}00152\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00153}00153\ \ \ \ \ \ \ \ \ \ \ \ \ use\_fast\ =\ \textcolor{keyword}{False}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00154}00154\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{'mpt'}\ \textcolor{keywordflow}{in}\ model\_name.lower():}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00155}00155\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_path,\ use\_fast=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00156}00156\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ AutoModelForCausalLM.from\_pretrained(model\_path,\ low\_cpu\_mem\_usage=\textcolor{keyword}{True},\ trust\_remote\_code=\textcolor{keyword}{True},\ **kwargs)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00157}00157\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00158}00158\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer\ =\ AutoTokenizer.from\_pretrained(model\_path,\ use\_fast=\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00159}00159\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ model\ =\ AutoModelForCausalLM.from\_pretrained(model\_path,\ low\_cpu\_mem\_usage=\textcolor{keyword}{True},\ **kwargs)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00160}00160\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00161}00161\ \ \ \ \ image\_processor\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00162}00162\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00163}00163\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{'llava'}\ \textcolor{keywordflow}{in}\ model\_name.lower():}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00164}00164\ \ \ \ \ \ \ \ \ mm\_use\_im\_start\_end\ =\ getattr(model.config,\ \textcolor{stringliteral}{"{}mm\_use\_im\_start\_end"{}},\ \textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00165}00165\ \ \ \ \ \ \ \ \ mm\_use\_im\_patch\_token\ =\ getattr(model.config,\ \textcolor{stringliteral}{"{}mm\_use\_im\_patch\_token"{}},\ \textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00166}00166\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ mm\_use\_im\_patch\_token:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00167}00167\ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer.add\_tokens([DEFAULT\_IMAGE\_PATCH\_TOKEN],\ special\_tokens=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00168}00168\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ mm\_use\_im\_start\_end:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00169}00169\ \ \ \ \ \ \ \ \ \ \ \ \ tokenizer.add\_tokens([DEFAULT\_IM\_START\_TOKEN,\ DEFAULT\_IM\_END\_TOKEN],\ special\_tokens=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00170}00170\ \ \ \ \ \ \ \ \ model.resize\_token\_embeddings(len(tokenizer))}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00171}00171\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00172}00172\ \ \ \ \ \ \ \ \ vision\_tower\ =\ model.get\_vision\_tower()}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00173}00173\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ vision\_tower.is\_loaded:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00174}00174\ \ \ \ \ \ \ \ \ \ \ \ \ vision\_tower.load\_model(device\_map=device\_map)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00175}00175\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ device\_map\ !=\ \textcolor{stringliteral}{'auto'}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00176}00176\ \ \ \ \ \ \ \ \ \ \ \ \ vision\_tower.to(device=device\_map,\ dtype=torch.float16)}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00177}00177\ \ \ \ \ \ \ \ \ image\_processor\ =\ vision\_tower.image\_processor}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00178}00178\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00179}00179\ \ \ \ \ \textcolor{keywordflow}{if}\ hasattr(model.config,\ \textcolor{stringliteral}{"{}max\_sequence\_length"{}}):}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00180}00180\ \ \ \ \ \ \ \ \ context\_len\ =\ model.config.max\_sequence\_length}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00181}00181\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00182}00182\ \ \ \ \ \ \ \ \ context\_len\ =\ 2048}
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00183}00183\ }
\DoxyCodeLine{\Hypertarget{builder_8py_source_l00184}00184\ \ \ \ \ \textcolor{keywordflow}{return}\ tokenizer,\ model,\ image\_processor,\ context\_len}

\end{DoxyCode}
